{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb28321-238b-4db2-b4ef-c36dc08ec153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score\n",
    "import datetime\n",
    "import time\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForMaskedLM\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import word2vec, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad166e6-d167-471d-914e-2959eb3db314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 设置随机数种子\n",
    "setup_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ddf018-af0f-46bf-b4e3-aff355a1a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/root/autodl-fs/dataset/dataset_user_job_all_test1.csv', dtype = {'UserID': 'str', 'JobID': 'str','label': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7194e91-187f-4cf1-91cf-a886a5d46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_job = dataset['skill_entity_en_job'].values\n",
    "skill_user = dataset['skill_entity_en_user'].values\n",
    "\n",
    "skill_job_emb = []\n",
    "for skills in skill_job:\n",
    "    skill_job_emb.append(skills.split(','))\n",
    "dataset['skill_job'] = skill_job_emb\n",
    "\n",
    "skill_user_emb = []\n",
    "for skills in skill_user:\n",
    "    skill_user_emb.append(skills.split(','))\n",
    "dataset['skill_user'] = skill_user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeddd2d0-e037-4adf-a7de-1a4a0a0061e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = np.concatenate((skill_job_emb,skill_user_emb),axis=0)\n",
    "# w2v_model = word2vec.Word2Vec(text_array, size=100, window=5, min_count=2, workers=8, iter=10, sg=1)\n",
    "w2v_model = word2vec.Word2Vec(text_array, size=8, window=5, min_count=2, workers=8, iter=10, sg=1)\n",
    "w2v_model.save('word2vec_all.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6261cdc7-f7b9-438d-8726-0eb1084b9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('word2vec_all.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffe314b-5cf7-4406-82e8-684b22e85778",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_present_list = list(w2v_model.wv.index2word)\n",
    "dataset['skill_job'] = dataset['skill_job'].apply(lambda x:[i for i in x if i in word_present_list])\n",
    "dataset['skill_user'] = dataset['skill_user'].apply(lambda x:[i for i in x if i in word_present_list])\n",
    "# 先用列平均值，后面看情况再改\n",
    "dataset['skill_job'] = dataset['skill_job'].apply(lambda x: np.mean([np.array(w2v_model.wv[i]).reshape(1,8) for i in x], axis=0))\n",
    "dataset['skill_user'] = dataset['skill_user'].apply(lambda x: np.mean([np.array(w2v_model.wv[i]).reshape(1,8) for i in x], axis=0))\n",
    "\n",
    "skill_user_1 = []\n",
    "skill_user_2 = []\n",
    "skill_user_3 = []\n",
    "skill_user_4 = []\n",
    "skill_user_5 = []\n",
    "skill_user_6 = []\n",
    "skill_user_7 = []\n",
    "skill_user_8 = []\n",
    "\n",
    "skill_job_1 = []\n",
    "skill_job_2 = []\n",
    "skill_job_3 = []\n",
    "skill_job_4 = []\n",
    "skill_job_5 = []\n",
    "skill_job_6 = []\n",
    "skill_job_7 = []\n",
    "skill_job_8 = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        skill_job_embedding = dataset.loc[i, 'skill_job'][0].tolist()\n",
    "        skill_job_1.append(skill_job_embedding[0])\n",
    "        skill_job_2.append(skill_job_embedding[1])\n",
    "        skill_job_3.append(skill_job_embedding[2])\n",
    "        skill_job_4.append(skill_job_embedding[3])\n",
    "        skill_job_5.append(skill_job_embedding[4])\n",
    "        skill_job_6.append(skill_job_embedding[5])\n",
    "        skill_job_7.append(skill_job_embedding[6])\n",
    "        skill_job_8.append(skill_job_embedding[7])\n",
    "    except:\n",
    "        skill_job_1.append(0)\n",
    "        skill_job_2.append(0)\n",
    "        skill_job_3.append(0)\n",
    "        skill_job_4.append(0)\n",
    "        skill_job_5.append(0)\n",
    "        skill_job_6.append(0)\n",
    "        skill_job_7.append(0)\n",
    "        skill_job_8.append(0)\n",
    "    skill_user_embedding = dataset.loc[i, 'skill_user'][0].tolist()\n",
    "    skill_user_1.append(skill_user_embedding[0])\n",
    "    skill_user_2.append(skill_user_embedding[1])\n",
    "    skill_user_3.append(skill_user_embedding[2])\n",
    "    skill_user_4.append(skill_user_embedding[3])\n",
    "    skill_user_5.append(skill_user_embedding[4])\n",
    "    skill_user_6.append(skill_user_embedding[5])\n",
    "    skill_user_7.append(skill_user_embedding[6])\n",
    "    skill_user_8.append(skill_user_embedding[7])\n",
    "\n",
    "\n",
    "dataset['skill_user_1'] = skill_user_1\n",
    "dataset['skill_user_2'] = skill_user_2\n",
    "dataset['skill_user_3'] = skill_user_3\n",
    "dataset['skill_user_4'] = skill_user_4\n",
    "dataset['skill_user_5'] = skill_user_5\n",
    "dataset['skill_user_6'] = skill_user_6\n",
    "dataset['skill_user_7'] = skill_user_7\n",
    "dataset['skill_user_8'] = skill_user_8\n",
    "\n",
    "dataset['skill_job_1'] = skill_job_1\n",
    "dataset['skill_job_2'] = skill_job_2\n",
    "dataset['skill_job_3'] = skill_job_3\n",
    "dataset['skill_job_4'] = skill_job_4\n",
    "dataset['skill_job_5'] = skill_job_5\n",
    "dataset['skill_job_6'] = skill_job_6\n",
    "dataset['skill_job_7'] = skill_job_7\n",
    "dataset['skill_job_8'] = skill_job_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb9726d-bf7d-4161-987b-2f23706b9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稠密特征\n",
    "dense_feas = ['岗位薪资下限(K)','岗位薪资上限(K)','work_length','总分','岗位招聘人数']\n",
    "\n",
    "# 文本特征\n",
    "vec_feas = ['skill_job_1','skill_job_2','skill_job_3','skill_job_4',\n",
    "            'skill_job_5','skill_job_6','skill_job_7','skill_job_8',\n",
    "            'skill_user_1','skill_user_2','skill_user_3','skill_user_4',\n",
    "            'skill_user_5','skill_user_6','skill_user_7','skill_user_8']\n",
    "\n",
    "text_feas = ['岗位名称', '岗位描述', 'experience']\n",
    "\n",
    "# 稀疏特征\n",
    "## TODO userid jobid应该放进去吗\n",
    "sparse_feas_user = ['UserID','性别','专业']\n",
    "sparse_feas_job = ['JobID','企业融资阶段','企业人员规模','企业休息时间','企业加班情况','岗位一级类别','岗位三级类别','岗位工作经验','岗位招聘类型'] # '岗位二级类别'\n",
    "sparse_feas_match = ['match_degree','match_loc_job','match_loc_corp']\n",
    "sparse_feas = sparse_feas_user + sparse_feas_job + sparse_feas_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a216ad15-c327-481a-9221-17acdd5f5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseFeature(feat, feat_num, embed_dim=8):\n",
    "    # if len(dataset[feat].unique()) < embed_dim:\n",
    "    #     embed_dim = len(dataset[feat].unique())\n",
    "    return {'feat':feat, 'feat_num':feat_num, 'embed_dim':embed_dim}\n",
    "\n",
    "def denseFeature(feat):\n",
    "    return {'feat':feat}\n",
    "\n",
    "def vecFeature(feat):\n",
    "    return{'feat':feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c8c94ee-5164-41c9-995a-1be525d8fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "feature_columns = [[denseFeature(feat) for feat in dense_feas]] +[[sparseFeature(feat, len(dataset[feat].unique()), embed_dim=embed_dim) for feat in sparse_feas]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cc9ccd-ffcc-4ec1-8d53-c51d7adb9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        \"\"\"\n",
    "        latent_dim:各个离散特征隐向量的维度\n",
    "        fea_num:特征个数\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # print('fea_num',fea_num)   #82\n",
    "        #定义三个矩阵，一个是全局偏置，一个是一阶权重矩阵，一个是二阶交叉矩阵\n",
    "        self.w0 = nn.Parameter(torch.zeros([1,]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "    def forward(self, x):\n",
    "        #x的维度是(batch_size, fea_num)\n",
    "        #一阶交叉\n",
    "        # x=x[:,:82]   #[32,222]\n",
    "        # print(\"x\",x.shape)\n",
    "        first_order = self.w0 + torch.mm(x, self.w1) #(batch_size, 1)\n",
    "        #二阶交叉\n",
    "        second_order = 1/2 * torch.sum(torch.pow(torch.mm(x, self.w2), 2) - torch.mm(torch.pow(x, 2), torch.pow(self.w2, 2)), dim=1, keepdim=True)\n",
    "        return first_order + second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5515e3-e33e-4d45-8aec-d01eeec52475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dnn(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        \"\"\"\n",
    "        hidden_units:列表，每个元素表示每一层的神经单元个数，比如[256,128,64]两层网络，第一个维度是输入维度\n",
    "        \"\"\"\n",
    "        super(Dnn, self).__init__()\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        #layer[0]: (128,64)   layer[1]:(64,32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn_network:\n",
    "            # print(\"linear\",linear)\n",
    "            # print(\"x1\",x.shape)  # [32,102]\n",
    "            x = linear(x)\n",
    "            # print(\"x2\",x.shape)\n",
    "            x = F.relu(x)\n",
    "            # print(\"x2\", x.shape)\n",
    "        x = self.dropout(x)\n",
    "        # print(x,x.shape)   [32,32]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2200344f-0ead-4206-85b2-700ea06d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_columns, hidden_units, dnn_dropout=0.):\n",
    "        \"\"\"\n",
    "        feature_columns:特征信息\n",
    "        hidden_units:dnn的隐藏单元个数\n",
    "        dnn_dropout:失活率\n",
    "        \"\"\"\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        print(self.sparse_feature_cols)\n",
    "\n",
    "        # embedding\n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim']) for\n",
    "            i, feat in enumerate(self.sparse_feature_cols)    #len=26\n",
    "        })\n",
    "\n",
    "        self.fea_num = len(self.dense_feature_cols)\n",
    "        for one in self.sparse_feature_cols:\n",
    "            self.fea_num += one[\"embed_dim\"]\n",
    "        self.fea_num += len(vec_feas)\n",
    "        hidden_units.insert(0, self.fea_num)  #在hidden_units的最前面插入self.fea_num\n",
    "\n",
    "        self.fm = FM(self.sparse_feature_cols[0]['embed_dim'], self.fea_num)\n",
    "        self.dnn_network = Dnn(hidden_units, dnn_dropout)\n",
    "        self.nn_final_linear = nn.Linear(hidden_units[-1], 1)  #[32,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_inputs, sparse_inputs= x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):len(self.dense_feature_cols) + 15]\n",
    "        vec_inputs=x[:,len(self.dense_feature_cols) + 15:]\n",
    "        sparse_inputs = sparse_inputs.long()     #将数字或字符串转换成长整型\n",
    "        sparse_embeds = [self.embed_layers['embed_' + str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]     #for i in range(10)   0-9\n",
    "        sparse_embeds = torch.cat(sparse_embeds, dim=-1)\n",
    "\n",
    "        # 把离散特征、连续特征、文本向量 拼接作为FM和DNN的输入\n",
    "        x = torch.cat([sparse_embeds, dense_inputs, vec_inputs], dim=-1)\n",
    "        # Wide\n",
    "        wide_outputs = self.fm(x)\n",
    "        # deep\n",
    "        deep_outputs = self.nn_final_linear(self.dnn_network(x))\n",
    "\n",
    "        # 模型的最后输出\n",
    "        outputs = torch.add(wide_outputs, deep_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f00e7c-2f5b-4117-8488-1618f9959704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobUserDataset(data.Dataset):\n",
    "    '''\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    '''\n",
    "    def __init__(self, job, user, deepfm, label):\n",
    "        self.job = job\n",
    "        self.user = user\n",
    "        self.deepfm = deepfm\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.job[idx], self.user[idx], self.deepfm[idx]\n",
    "        return self.job[idx], self.user[idx], self.deepfm[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "219e9f1f-2141-42f1-809d-4932c09ae0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1905f706-7607-40af-afc1-0af87fca0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (N, L, D)\n",
    "        a = self.attn(x)        # (N, L, 1)\n",
    "        x = (x * a).sum(dim=1)  # (N, D)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b32a06d0-77e2-4d5e-826f-9a7938eddcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_layer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttentionEncoder(dim) # * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.attn(x)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a7ae0c-28ca-41d5-bfce-7dc3aa9fdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAttentionEncoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dim, dim, bias=False)\n",
    "        self.U = nn.Linear(dim, dim, bias=False)\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim, 1, bias=False),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        # (N, L, D), (N, S1, D)\n",
    "        s = s.permute(1, 0, 2)  # (S2, N, D)\n",
    "        # calculate co-attention score\n",
    "        y = torch.cat([self.attn(self.W(x.permute(1, 0, 2)) + self.U( _.expand(x.shape[1], _.shape[0], _.shape[1]) ) ).permute(2, 0, 1) for _ in s ]).permute(2, 0, 1)\n",
    "        # (N, D) -> (L, N, D) -> (L, N, 1) -- softmax as L --> (L, N, 1) -> (1, L, N) -> (S2, L, N) -> (N, S2, L)\n",
    "        sr = torch.cat([torch.mm(y[i], _).unsqueeze(0) for i, _ in enumerate(x)])   # (N, S2, D)\n",
    "        sr = torch.sum(sr, dim=1)  # (N, D)\n",
    "        return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f8f634b-7d6f-40fb-b43b-e41405d7207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAttention_layer(nn.Module):\n",
    "    def __init__(self, dim, hd_size):\n",
    "        super().__init__()\n",
    "        self.co_attn = CoAttentionEncoder(dim)\n",
    "        self.biLSTM = nn.LSTM(\n",
    "            input_size=dim,\n",
    "            hidden_size=hd_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.self_attn = SelfAttentionEncoder(dim) # * 2\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        # x: self\n",
    "        # s: other\n",
    "        # (N, S1, D), (N, S2, D)\n",
    "        s = s.unsqueeze(2)\n",
    "        s = s.permute(1, 0, 2, 3)  # (S2, N, L, D)\n",
    "        sr = torch.cat([self.co_attn(x, _).unsqueeze(0) for _ in s])   # (S1, N, D)\n",
    "        c = sr.permute(1, 0, 2)     # (N, S1, D)\n",
    "        # c = self.biLSTM(u)[0]       # (N, S1, D)\n",
    "        g = self.self_attn(c)       # (N, D)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2102e626-3641-4144-a640-988da5e39eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APJFFF_text(nn.Module):\n",
    "    def __init__(self, lstm_dim, lstm_hd_size, num_lstm_layers, dropout): #, dropout\n",
    "        super(APJFFF_text, self).__init__()\n",
    "        '''\n",
    "        APJFFF setting\n",
    "        '''\n",
    "        self.user_biLSTM = nn.LSTM(\n",
    "            input_size=lstm_dim,\n",
    "            hidden_size=lstm_hd_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.job_biLSTM = nn.LSTM(\n",
    "            input_size=lstm_dim,\n",
    "            hidden_size=lstm_hd_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "\n",
    "        self.job_layer_1 = Attention_layer(lstm_hd_size * 2) #, lstm_hd_size\n",
    "        self.job_layer_2 = CoAttention_layer(lstm_hd_size * 2, lstm_hd_size)\n",
    "\n",
    "        self.user_layer_1 = Attention_layer(lstm_hd_size * 2 ) #, lstm_hd_size\n",
    "        self.user_layer_2 = CoAttention_layer(lstm_hd_size * 2 , lstm_hd_size)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_size=lstm_hd_size * 2 * 4,\n",
    "            output_size=1,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.liner_layer = nn.Linear(lstm_hd_size * 2 * 8, 1)\n",
    "        # self.pool_layer = nn.AdaptiveAvgPool2d((1))\n",
    "\n",
    "    def forward(self, job, user):\n",
    "        # print('usersize',user.size()) # torch.Size([128, 10, 768])\n",
    "\n",
    "        # LSTM part\n",
    "        user_vecs = self.user_biLSTM(user)[0] #.unsqueeze(2)\n",
    "        # print('usersize', user_vecs.size()) # torch.Size([128, 1, 10, 128])\n",
    "        job_vecs = self.job_biLSTM(job)[0] #.unsqueeze(2)\n",
    "\n",
    "        # attention part: bilstm + attention\n",
    "        gj = self.job_layer_1(job_vecs)\n",
    "        gr = self.user_layer_1(user_vecs)\n",
    "\n",
    "        # coAttention part\n",
    "        # gjj = self.job_layer_2(job_vecs,user_vecs)\n",
    "        # grr = self.user_layer_2(user_vecs, job_vecs)\n",
    "\n",
    "        # concat the vectors\n",
    "        # x = torch.cat([gjj, grr, gjj - grr, gjj * grr, gj , gr, gj - gr, gj * gr], axis=1)\n",
    "        # x = torch.cat([gjj, grr, gjj - grr, gjj * grr], axis=1)\n",
    "        x = torch.cat([gj , gr, gj - gr, gj * gr], axis=1)\n",
    "        # print('x_size1:', x.size()) # x_size1: torch.Size([128, 1024])\n",
    "\n",
    "        # fully connected layer\n",
    "        # x = self.liner_layer(x)\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbf39f9f-2467-46ea-a040-bdcb6000c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APJFF_connect(nn.Module):\n",
    "    def __init__(self, lstm_dim, lstm_hd_size, num_lstm_layers, vec_size_2, dropout, feature_columns, hidden_units, dnn_dropout):\n",
    "        super(APJFF_connect, self).__init__()\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_size=1, # 2049\n",
    "            output_size=1,\n",
    "            dropout= 0.7 #dropout\n",
    "        )\n",
    "        self.atten_layer = Attention_layer(1)\n",
    "        self.linear_layer = nn.Linear(2,1)\n",
    "        self.text_layer = APJFFF_text(lstm_dim, lstm_hd_size, num_lstm_layers, dropout)\n",
    "        self.entity_layer = DeepFM(feature_columns, hidden_units, dnn_dropout)\n",
    "\n",
    "    def forward(self, job, user, x):\n",
    "        text_vec = self.text_layer(job, user)\n",
    "        entity_vec = self.entity_layer(x)\n",
    "\n",
    "        # attention\n",
    "        vec = torch.stack([text_vec, entity_vec]) # (N, S, D) -> (N, D)\n",
    "        vec = vec.permute(1, 0, 2) # 2, 1, 0, 3\n",
    "        vec_att = self.atten_layer(vec)\n",
    "        x = self.mlp(vec_att)\n",
    "        x = x.squeeze(1)\n",
    "        # x = F.sigmoid(vec_att).squeeze()\n",
    "\n",
    "        # no attention\n",
    "        # vec = entity_vec\n",
    "        # vec = torch.cat([text_vec,entity_vec], axis=1)\n",
    "        # print('vecsize',vec.size())\n",
    "        # x = self.mlp(vec).squeeze(1)\n",
    "        # x = F.sigmoid(vec).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d9a7d5-fac5-41ed-920e-9f789daecbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(n_epoch, lr, train, valid, model, device, model_name, model_dir=\"./\"):\n",
    "    # summary model parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\nstart training, total parameter:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch, eta_min=0, last_epoch=-1)\n",
    "    # total_loss, total_acc = 0, 0\n",
    "    best_acc, best_precision, best_recall, best_f1, best_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        pred_label = []\n",
    "        y_label = []\n",
    "        # training\n",
    "        for i, (jobs, users, entities, labels) in enumerate(train):\n",
    "\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            entities = entities.to(torch.float32)\n",
    "            entities = entities.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO 是否考虑模型用多个优化器？\n",
    "            optimizer.zero_grad() # 将所有模型参数的梯度置为0\n",
    "            # model.zero_grad() # 除所有可训练的torch.Tensor的梯度\n",
    "            outputs = model(jobs, users, entities)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_label.extend([0 if i<0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "        train_losses = total_loss/t_batch\n",
    "        train_acc = accuracy_score(y_label, pred_label)\n",
    "        train_precision = precision_score(y_label, pred_label)\n",
    "        train_recall = recall_score(y_label, pred_label)\n",
    "        train_auc = roc_auc_score(y_label, pred_label)\n",
    "        train_f1 = f1_score(y_label, pred_label)\n",
    "        print('[ Epoch{}: {}/{}] '.format(epoch+1, i+1, t_batch))\n",
    "        # print('\\nTrain | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(train_losses,train_acc,train_precision, train_recall,train_auc,train_f1, time.time()-start_time))\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # pred_score = []\n",
    "            pred_label = []\n",
    "            y_label = []\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (jobs, users, entities, labels) in enumerate(valid):\n",
    "                # 放GPU上运行\n",
    "                jobs = jobs.to(torch.float32)\n",
    "                jobs = jobs.to(device)\n",
    "\n",
    "                users = users.to(torch.float32)\n",
    "                users = users.to(device)\n",
    "\n",
    "                entities = entities.to(torch.float32)\n",
    "                entities = entities.to(device)\n",
    "\n",
    "                labels = labels.to(torch.float32)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(jobs, users, entities)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                '''\n",
    "                存一下预测score\n",
    "                '''\n",
    "                # pred_score.extend([j for j in list(outputs.cpu().detach().numpy())])\n",
    "                pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "                y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "            val_losses = total_loss/v_batch\n",
    "            val_acc = accuracy_score(y_label, pred_label)\n",
    "            val_precision = precision_score(y_label, pred_label)\n",
    "            val_recall = recall_score(y_label, pred_label)\n",
    "            val_auc = roc_auc_score(y_label, pred_label)\n",
    "            val_f1 = f1_score(y_label, pred_label)\n",
    "            print('\\nVal | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(val_losses,val_acc,val_precision, val_recall,val_auc,val_f1, time.time()-start_time))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_precision = val_precision\n",
    "                best_recall = val_recall\n",
    "                best_f1 = val_f1\n",
    "                best_auc = val_auc\n",
    "                torch.save(model, \"{}/{}.model\".format(model_dir, model_name))\n",
    "                print('save model with acc: {:.3f}, recall: {:.3f}, auc: {:.3f}'.format(best_acc,best_recall,best_auc))\n",
    "        print('------------------------------------------------------')\n",
    "        lr_scheduler.step()\n",
    "        model.train()\n",
    "    return best_acc, best_precision, best_recall, best_f1, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc6f5931-a2ac-44a7-9d73-d83c8064217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'feat': 'UserID', 'feat_num': 7921, 'embed_dim': 8}, {'feat': '性别', 'feat_num': 2, 'embed_dim': 8}, {'feat': '专业', 'feat_num': 935, 'embed_dim': 8}, {'feat': 'JobID', 'feat_num': 31647, 'embed_dim': 8}, {'feat': '企业融资阶段', 'feat_num': 9, 'embed_dim': 8}, {'feat': '企业人员规模', 'feat_num': 7, 'embed_dim': 8}, {'feat': '企业休息时间', 'feat_num': 4, 'embed_dim': 8}, {'feat': '企业加班情况', 'feat_num': 3, 'embed_dim': 8}, {'feat': '岗位一级类别', 'feat_num': 21, 'embed_dim': 8}, {'feat': '岗位三级类别', 'feat_num': 530, 'embed_dim': 8}, {'feat': '岗位工作经验', 'feat_num': 5, 'embed_dim': 8}, {'feat': '岗位招聘类型', 'feat_num': 4, 'embed_dim': 8}, {'feat': 'match_degree', 'feat_num': 2, 'embed_dim': 8}, {'feat': 'match_loc_job', 'feat_num': 2, 'embed_dim': 8}, {'feat': 'match_loc_corp', 'feat_num': 2, 'embed_dim': 8}]\n"
     ]
    }
   ],
   "source": [
    "# 设置模型参数\n",
    "hidden_units = [128, 64, 32]\n",
    "dnn_dropout = 0.7\n",
    "\n",
    "\n",
    "lstm_dim = 768\n",
    "lstm_hd_size = 128\n",
    "num_lstm_layers = 1\n",
    "dropout = 0.7\n",
    "vec_size_2 = 32\n",
    "\n",
    "# 训练参数\n",
    "epoch = 100\n",
    "lr = 0.005\n",
    "# batch_size = 4012\n",
    "batch_size = 32\n",
    "model_dir = './' # change with your model save path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"APJFFF_coatt\"\n",
    "# 定义模型\n",
    "APJFFF_model = APJFF_connect(lstm_dim, lstm_hd_size, num_lstm_layers, vec_size_2, dropout, feature_columns, hidden_units, dnn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0086944-9fe6-40e8-abfc-56f889f02c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(\"/root/autodl-fs/apjfff_dataset/train_3.dataset\")\n",
    "val_dataset = torch.load(\"/root/autodl-fs/apjfff_dataset/val_3.dataset\")\n",
    "test_dataset = torch.load(\"/root/autodl-fs/apjfff_dataset/test_3.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3f08e0c-12c4-4a6a-a019-7099b7edb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader导入\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8481161e-6e4a-40fb-b003-1d8585310ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, total parameter:4567731, trainable:4567731\n",
      "\n",
      "[ Epoch1: 1766/1766] \n",
      "\n",
      "Val | Loss:0.68191 ACC:0.55609 Precision:0.71473 Recall:0.19379 AUC:0.55785 F1:0.30490 Time:43.853601\n",
      "save model with acc: 0.556, recall: 0.194, auc: 0.558\n",
      "------------------------------------------------------\n",
      "[ Epoch2: 1766/1766] \n",
      "\n",
      "Val | Loss:0.66432 ACC:0.60583 Precision:0.85216 Recall:0.26067 AUC:0.60750 F1:0.39922 Time:44.027925\n",
      "save model with acc: 0.606, recall: 0.261, auc: 0.608\n",
      "------------------------------------------------------\n",
      "[ Epoch3: 1766/1766] \n",
      "\n",
      "Val | Loss:0.61258 ACC:0.70287 Precision:0.84167 Recall:0.50328 AUC:0.70384 F1:0.62990 Time:43.721634\n",
      "save model with acc: 0.703, recall: 0.503, auc: 0.704\n",
      "------------------------------------------------------\n",
      "[ Epoch4: 1766/1766] \n",
      "\n",
      "Val | Loss:0.57783 ACC:0.73764 Precision:0.83888 Recall:0.59140 AUC:0.73835 F1:0.69373 Time:43.535789\n",
      "save model with acc: 0.738, recall: 0.591, auc: 0.738\n",
      "------------------------------------------------------\n",
      "[ Epoch5: 1766/1766] \n",
      "\n",
      "Val | Loss:0.53468 ACC:0.76806 Precision:0.86274 Recall:0.64022 AUC:0.76868 F1:0.73500 Time:43.205227\n",
      "save model with acc: 0.768, recall: 0.640, auc: 0.769\n",
      "------------------------------------------------------\n",
      "[ Epoch6: 1766/1766] \n",
      "\n",
      "Val | Loss:0.50395 ACC:0.78988 Precision:0.83089 Recall:0.73045 AUC:0.79017 F1:0.77744 Time:44.272180\n",
      "save model with acc: 0.790, recall: 0.730, auc: 0.790\n",
      "------------------------------------------------------\n",
      "[ Epoch7: 1766/1766] \n",
      "\n",
      "Val | Loss:0.49171 ACC:0.78855 Precision:0.77956 Recall:0.80748 AUC:0.78846 F1:0.79327 Time:44.213286\n",
      "------------------------------------------------------\n",
      "[ Epoch8: 1766/1766] \n",
      "\n",
      "Val | Loss:0.49601 ACC:0.79965 Precision:0.82860 Recall:0.75803 AUC:0.79985 F1:0.79174 Time:44.309774\n",
      "save model with acc: 0.800, recall: 0.758, auc: 0.800\n",
      "------------------------------------------------------\n",
      "[ Epoch9: 1766/1766] \n",
      "\n",
      "Val | Loss:0.47620 ACC:0.79402 Precision:0.76800 Recall:0.84541 AUC:0.79377 F1:0.80485 Time:44.061615\n",
      "------------------------------------------------------\n",
      "[ Epoch10: 1766/1766] \n",
      "\n",
      "Val | Loss:0.47404 ACC:0.80867 Precision:0.86772 Recall:0.73056 AUC:0.80905 F1:0.79325 Time:43.527854\n",
      "save model with acc: 0.809, recall: 0.731, auc: 0.809\n",
      "------------------------------------------------------\n",
      "[ Epoch11: 1766/1766] \n",
      "\n",
      "Val | Loss:0.47322 ACC:0.80252 Precision:0.79229 Recall:0.82259 AUC:0.80242 F1:0.80715 Time:43.417572\n",
      "------------------------------------------------------\n",
      "[ Epoch12: 1766/1766] \n",
      "\n",
      "Val | Loss:0.43331 ACC:0.81510 Precision:0.82460 Recall:0.80273 AUC:0.81516 F1:0.81351 Time:42.915381\n",
      "save model with acc: 0.815, recall: 0.803, auc: 0.815\n",
      "------------------------------------------------------\n",
      "[ Epoch13: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44159 ACC:0.81037 Precision:0.79933 Recall:0.83126 AUC:0.81027 F1:0.81498 Time:43.615675\n",
      "------------------------------------------------------\n",
      "[ Epoch14: 1766/1766] \n",
      "\n",
      "Val | Loss:0.45521 ACC:0.81526 Precision:0.80088 Recall:0.84150 AUC:0.81513 F1:0.82069 Time:43.862565\n",
      "save model with acc: 0.815, recall: 0.842, auc: 0.815\n",
      "------------------------------------------------------\n",
      "[ Epoch15: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42597 ACC:0.82476 Precision:0.84973 Recall:0.79110 AUC:0.82492 F1:0.81937 Time:43.549338\n",
      "save model with acc: 0.825, recall: 0.791, auc: 0.825\n",
      "------------------------------------------------------\n",
      "[ Epoch16: 1766/1766] \n",
      "\n",
      "Val | Loss:0.43897 ACC:0.81807 Precision:0.80182 Recall:0.84732 AUC:0.81793 F1:0.82394 Time:41.705136\n",
      "------------------------------------------------------\n",
      "[ Epoch17: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42402 ACC:0.82651 Precision:0.82318 Recall:0.83379 AUC:0.82648 F1:0.82845 Time:43.552091\n",
      "save model with acc: 0.827, recall: 0.834, auc: 0.826\n",
      "------------------------------------------------------\n",
      "[ Epoch18: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44849 ACC:0.81573 Precision:0.78966 Recall:0.86317 AUC:0.81550 F1:0.82478 Time:43.881946\n",
      "------------------------------------------------------\n",
      "[ Epoch19: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44151 ACC:0.82364 Precision:0.80772 Recall:0.85175 AUC:0.82351 F1:0.82915 Time:43.592584\n",
      "------------------------------------------------------\n",
      "[ Epoch20: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41516 ACC:0.82152 Precision:0.80975 Recall:0.84277 AUC:0.82142 F1:0.82593 Time:43.635978\n",
      "------------------------------------------------------\n",
      "[ Epoch21: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41227 ACC:0.82832 Precision:0.83932 Recall:0.81414 AUC:0.82839 F1:0.82654 Time:44.098508\n",
      "save model with acc: 0.828, recall: 0.814, auc: 0.828\n",
      "------------------------------------------------------\n",
      "[ Epoch22: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41352 ACC:0.83225 Precision:0.83369 Recall:0.83210 AUC:0.83225 F1:0.83289 Time:43.243953\n",
      "save model with acc: 0.832, recall: 0.832, auc: 0.832\n",
      "------------------------------------------------------\n",
      "[ Epoch23: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40986 ACC:0.82715 Precision:0.81829 Recall:0.84320 AUC:0.82707 F1:0.83056 Time:44.126914\n",
      "------------------------------------------------------\n",
      "[ Epoch24: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40701 ACC:0.83140 Precision:0.83828 Recall:0.82322 AUC:0.83144 F1:0.83069 Time:43.735008\n",
      "------------------------------------------------------\n",
      "[ Epoch25: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42275 ACC:0.82672 Precision:0.85822 Recall:0.78476 AUC:0.82693 F1:0.81985 Time:43.999974\n",
      "------------------------------------------------------\n",
      "[ Epoch26: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42841 ACC:0.83076 Precision:0.86003 Recall:0.79205 AUC:0.83095 F1:0.82464 Time:43.963686\n",
      "------------------------------------------------------\n",
      "[ Epoch27: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42272 ACC:0.83437 Precision:0.84827 Recall:0.81636 AUC:0.83446 F1:0.83201 Time:43.888914\n",
      "save model with acc: 0.834, recall: 0.816, auc: 0.834\n",
      "------------------------------------------------------\n",
      "[ Epoch28: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41160 ACC:0.83150 Precision:0.83161 Recall:0.83337 AUC:0.83149 F1:0.83249 Time:43.312136\n",
      "------------------------------------------------------\n",
      "[ Epoch29: 1766/1766] \n",
      "\n",
      "Val | Loss:0.43876 ACC:0.83516 Precision:0.83944 Recall:0.83083 AUC:0.83519 F1:0.83511 Time:44.176687\n",
      "save model with acc: 0.835, recall: 0.831, auc: 0.835\n",
      "------------------------------------------------------\n",
      "[ Epoch30: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39728 ACC:0.83140 Precision:0.83018 Recall:0.83527 AUC:0.83138 F1:0.83272 Time:44.165351\n",
      "------------------------------------------------------\n",
      "[ Epoch31: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40690 ACC:0.83665 Precision:0.84951 Recall:0.82016 AUC:0.83673 F1:0.83458 Time:43.676029\n",
      "save model with acc: 0.837, recall: 0.820, auc: 0.837\n",
      "------------------------------------------------------\n",
      "[ Epoch32: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40952 ACC:0.83240 Precision:0.85270 Recall:0.80558 AUC:0.83253 F1:0.82847 Time:43.457198\n",
      "------------------------------------------------------\n",
      "[ Epoch33: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44882 ACC:0.83017 Precision:0.80944 Recall:0.86581 AUC:0.83000 F1:0.83668 Time:44.127308\n",
      "------------------------------------------------------\n",
      "[ Epoch34: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41283 ACC:0.83447 Precision:0.85580 Recall:0.80642 AUC:0.83461 F1:0.83038 Time:44.084735\n",
      "------------------------------------------------------\n",
      "[ Epoch35: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40066 ACC:0.83559 Precision:0.86172 Recall:0.80135 AUC:0.83576 F1:0.83044 Time:43.778901\n",
      "------------------------------------------------------\n",
      "[ Epoch36: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42079 ACC:0.83394 Precision:0.83796 Recall:0.82999 AUC:0.83396 F1:0.83395 Time:44.189529\n",
      "------------------------------------------------------\n",
      "[ Epoch37: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41115 ACC:0.83702 Precision:0.84697 Recall:0.82460 AUC:0.83708 F1:0.83564 Time:43.592918\n",
      "save model with acc: 0.837, recall: 0.825, auc: 0.837\n",
      "------------------------------------------------------\n",
      "[ Epoch38: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40064 ACC:0.83389 Precision:0.83129 Recall:0.83981 AUC:0.83386 F1:0.83553 Time:43.619365\n",
      "------------------------------------------------------\n",
      "[ Epoch39: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40107 ACC:0.83692 Precision:0.84167 Recall:0.83189 AUC:0.83694 F1:0.83675 Time:43.439003\n",
      "------------------------------------------------------\n",
      "[ Epoch40: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40696 ACC:0.83585 Precision:0.85574 Recall:0.80981 AUC:0.83598 F1:0.83214 Time:43.372024\n",
      "------------------------------------------------------\n",
      "[ Epoch41: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40214 ACC:0.83729 Precision:0.84902 Recall:0.82238 AUC:0.83736 F1:0.83549 Time:44.117600\n",
      "save model with acc: 0.837, recall: 0.822, auc: 0.837\n",
      "------------------------------------------------------\n",
      "[ Epoch42: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40012 ACC:0.83607 Precision:0.86384 Recall:0.79977 AUC:0.83624 F1:0.83057 Time:42.895784\n",
      "------------------------------------------------------\n",
      "[ Epoch43: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39712 ACC:0.83633 Precision:0.86039 Recall:0.80484 AUC:0.83649 F1:0.83169 Time:42.294290\n",
      "------------------------------------------------------\n",
      "[ Epoch44: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40272 ACC:0.83814 Precision:0.85730 Recall:0.81319 AUC:0.83826 F1:0.83466 Time:43.102859\n",
      "save model with acc: 0.838, recall: 0.813, auc: 0.838\n",
      "------------------------------------------------------\n",
      "[ Epoch45: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40015 ACC:0.84164 Precision:0.84736 Recall:0.83527 AUC:0.84167 F1:0.84127 Time:44.044825\n",
      "save model with acc: 0.842, recall: 0.835, auc: 0.842\n",
      "------------------------------------------------------\n",
      "[ Epoch46: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41225 ACC:0.83309 Precision:0.88072 Recall:0.77240 AUC:0.83339 F1:0.82301 Time:43.913473\n",
      "------------------------------------------------------\n",
      "[ Epoch47: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38719 ACC:0.83872 Precision:0.84232 Recall:0.83538 AUC:0.83874 F1:0.83883 Time:44.217278\n",
      "------------------------------------------------------\n",
      "[ Epoch48: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39955 ACC:0.83883 Precision:0.86473 Recall:0.80516 AUC:0.83899 F1:0.83388 Time:42.596849\n",
      "------------------------------------------------------\n",
      "[ Epoch49: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38357 ACC:0.83771 Precision:0.86096 Recall:0.80738 AUC:0.83786 F1:0.83331 Time:43.255634\n",
      "------------------------------------------------------\n",
      "[ Epoch50: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39854 ACC:0.83909 Precision:0.84560 Recall:0.83157 AUC:0.83913 F1:0.83853 Time:43.923374\n",
      "------------------------------------------------------\n",
      "[ Epoch51: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38428 ACC:0.84132 Precision:0.85691 Recall:0.82132 AUC:0.84142 F1:0.83874 Time:43.393345\n",
      "------------------------------------------------------\n",
      "[ Epoch52: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40344 ACC:0.83899 Precision:0.85519 Recall:0.81805 AUC:0.83909 F1:0.83620 Time:43.534260\n",
      "------------------------------------------------------\n",
      "[ Epoch53: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38425 ACC:0.83814 Precision:0.84133 Recall:0.83538 AUC:0.83815 F1:0.83834 Time:43.359643\n",
      "------------------------------------------------------\n",
      "[ Epoch54: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40033 ACC:0.83989 Precision:0.84824 Recall:0.82978 AUC:0.83994 F1:0.83891 Time:43.596502\n",
      "------------------------------------------------------\n",
      "[ Epoch55: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38121 ACC:0.84079 Precision:0.84710 Recall:0.83358 AUC:0.84083 F1:0.84028 Time:43.438263\n",
      "------------------------------------------------------\n",
      "[ Epoch56: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38846 ACC:0.84037 Precision:0.85482 Recall:0.82185 AUC:0.84046 F1:0.83801 Time:44.054243\n",
      "------------------------------------------------------\n",
      "[ Epoch57: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38978 ACC:0.83877 Precision:0.85773 Recall:0.81414 AUC:0.83889 F1:0.83537 Time:44.056010\n",
      "------------------------------------------------------\n",
      "[ Epoch58: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38136 ACC:0.84106 Precision:0.85409 Recall:0.82449 AUC:0.84114 F1:0.83903 Time:43.343160\n",
      "------------------------------------------------------\n",
      "[ Epoch59: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37739 ACC:0.84207 Precision:0.85950 Recall:0.81963 AUC:0.84218 F1:0.83909 Time:44.005603\n",
      "save model with acc: 0.842, recall: 0.820, auc: 0.842\n",
      "------------------------------------------------------\n",
      "[ Epoch60: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37587 ACC:0.84164 Precision:0.86736 Recall:0.80843 AUC:0.84180 F1:0.83686 Time:43.967239\n",
      "------------------------------------------------------\n",
      "[ Epoch61: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37825 ACC:0.84148 Precision:0.84435 Recall:0.83918 AUC:0.84149 F1:0.84176 Time:44.097735\n",
      "------------------------------------------------------\n",
      "[ Epoch62: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37597 ACC:0.84127 Precision:0.85501 Recall:0.82375 AUC:0.84135 F1:0.83909 Time:43.758471\n",
      "------------------------------------------------------\n",
      "[ Epoch63: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38084 ACC:0.84260 Precision:0.85363 Recall:0.82883 AUC:0.84266 F1:0.84104 Time:44.280358\n",
      "save model with acc: 0.843, recall: 0.829, auc: 0.843\n",
      "------------------------------------------------------\n",
      "[ Epoch64: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37217 ACC:0.84313 Precision:0.86803 Recall:0.81107 AUC:0.84328 F1:0.83859 Time:44.482084\n",
      "save model with acc: 0.843, recall: 0.811, auc: 0.843\n",
      "------------------------------------------------------\n",
      "[ Epoch65: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37220 ACC:0.84499 Precision:0.84735 Recall:0.84341 AUC:0.84499 F1:0.84537 Time:44.364420\n",
      "save model with acc: 0.845, recall: 0.843, auc: 0.845\n",
      "------------------------------------------------------\n",
      "[ Epoch66: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37101 ACC:0.84392 Precision:0.85038 Recall:0.83654 AUC:0.84396 F1:0.84340 Time:44.168277\n",
      "------------------------------------------------------\n",
      "[ Epoch67: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36791 ACC:0.84307 Precision:0.85586 Recall:0.82692 AUC:0.84315 F1:0.84114 Time:44.606376\n",
      "------------------------------------------------------\n",
      "[ Epoch68: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36874 ACC:0.84541 Precision:0.86008 Recall:0.82682 AUC:0.84550 F1:0.84312 Time:44.256787\n",
      "save model with acc: 0.845, recall: 0.827, auc: 0.846\n",
      "------------------------------------------------------\n",
      "[ Epoch69: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36707 ACC:0.84499 Precision:0.85838 Recall:0.82809 AUC:0.84507 F1:0.84296 Time:44.466328\n",
      "------------------------------------------------------\n",
      "[ Epoch70: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36570 ACC:0.84196 Precision:0.85506 Recall:0.82534 AUC:0.84204 F1:0.83994 Time:44.317648\n",
      "------------------------------------------------------\n",
      "[ Epoch71: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37472 ACC:0.84079 Precision:0.87452 Recall:0.79755 AUC:0.84100 F1:0.83426 Time:43.841809\n",
      "------------------------------------------------------\n",
      "[ Epoch72: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36813 ACC:0.84318 Precision:0.85738 Recall:0.82513 AUC:0.84327 F1:0.84094 Time:44.071731\n",
      "------------------------------------------------------\n",
      "[ Epoch73: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36832 ACC:0.84249 Precision:0.85749 Recall:0.82333 AUC:0.84258 F1:0.84006 Time:44.652654\n",
      "------------------------------------------------------\n",
      "[ Epoch74: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36760 ACC:0.84366 Precision:0.86768 Recall:0.81276 AUC:0.84381 F1:0.83933 Time:43.813640\n",
      "------------------------------------------------------\n",
      "[ Epoch75: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37005 ACC:0.84116 Precision:0.85932 Recall:0.81773 AUC:0.84128 F1:0.83801 Time:44.227605\n",
      "------------------------------------------------------\n",
      "[ Epoch76: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36898 ACC:0.84116 Precision:0.86156 Recall:0.81477 AUC:0.84129 F1:0.83751 Time:44.013986\n",
      "------------------------------------------------------\n",
      "[ Epoch77: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36648 ACC:0.84292 Precision:0.85817 Recall:0.82344 AUC:0.84301 F1:0.84044 Time:44.448676\n",
      "------------------------------------------------------\n",
      "[ Epoch78: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36671 ACC:0.84223 Precision:0.85804 Recall:0.82196 AUC:0.84232 F1:0.83961 Time:44.093864\n",
      "------------------------------------------------------\n",
      "[ Epoch79: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36460 ACC:0.84376 Precision:0.86721 Recall:0.81361 AUC:0.84391 F1:0.83956 Time:43.757681\n",
      "------------------------------------------------------\n",
      "[ Epoch80: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36742 ACC:0.84430 Precision:0.87020 Recall:0.81107 AUC:0.84446 F1:0.83960 Time:44.676844\n",
      "------------------------------------------------------\n",
      "[ Epoch81: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36713 ACC:0.84132 Precision:0.86185 Recall:0.81477 AUC:0.84145 F1:0.83765 Time:44.053077\n",
      "------------------------------------------------------\n",
      "[ Epoch82: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36577 ACC:0.84185 Precision:0.86751 Recall:0.80875 AUC:0.84201 F1:0.83710 Time:44.029123\n",
      "------------------------------------------------------\n",
      "[ Epoch83: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36551 ACC:0.84297 Precision:0.86583 Recall:0.81350 AUC:0.84311 F1:0.83885 Time:44.370744\n",
      "------------------------------------------------------\n",
      "[ Epoch84: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36800 ACC:0.84169 Precision:0.87150 Recall:0.80336 AUC:0.84188 F1:0.83605 Time:44.609147\n",
      "------------------------------------------------------\n",
      "[ Epoch85: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36369 ACC:0.84191 Precision:0.86073 Recall:0.81762 AUC:0.84202 F1:0.83863 Time:44.547609\n",
      "------------------------------------------------------\n",
      "[ Epoch86: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36757 ACC:0.84047 Precision:0.86862 Recall:0.80410 AUC:0.84065 F1:0.83512 Time:44.372739\n",
      "------------------------------------------------------\n",
      "[ Epoch87: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36684 ACC:0.84090 Precision:0.86715 Recall:0.80695 AUC:0.84106 F1:0.83597 Time:44.410487\n",
      "------------------------------------------------------\n",
      "[ Epoch88: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36547 ACC:0.84201 Precision:0.85957 Recall:0.81942 AUC:0.84212 F1:0.83901 Time:44.547771\n",
      "------------------------------------------------------\n",
      "[ Epoch89: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36621 ACC:0.84228 Precision:0.86318 Recall:0.81530 AUC:0.84241 F1:0.83856 Time:44.129739\n",
      "------------------------------------------------------\n",
      "[ Epoch90: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36632 ACC:0.84249 Precision:0.85947 Recall:0.82069 AUC:0.84260 F1:0.83963 Time:44.378065\n",
      "------------------------------------------------------\n",
      "[ Epoch91: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36685 ACC:0.84154 Precision:0.86362 Recall:0.81298 AUC:0.84167 F1:0.83753 Time:44.600346\n",
      "------------------------------------------------------\n",
      "[ Epoch92: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36762 ACC:0.84021 Precision:0.86299 Recall:0.81065 AUC:0.84035 F1:0.83600 Time:44.364023\n",
      "------------------------------------------------------\n",
      "[ Epoch93: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36743 ACC:0.84021 Precision:0.86529 Recall:0.80769 AUC:0.84037 F1:0.83550 Time:44.350128\n",
      "------------------------------------------------------\n",
      "[ Epoch94: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36820 ACC:0.84063 Precision:0.86658 Recall:0.80706 AUC:0.84080 F1:0.83576 Time:44.387408\n",
      "------------------------------------------------------\n",
      "[ Epoch95: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36843 ACC:0.84074 Precision:0.86636 Recall:0.80759 AUC:0.84090 F1:0.83594 Time:44.681219\n",
      "------------------------------------------------------\n",
      "[ Epoch96: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36822 ACC:0.84079 Precision:0.86349 Recall:0.81139 AUC:0.84093 F1:0.83663 Time:44.196472\n",
      "------------------------------------------------------\n",
      "[ Epoch97: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36835 ACC:0.84010 Precision:0.86182 Recall:0.81192 AUC:0.84024 F1:0.83613 Time:44.501134\n",
      "------------------------------------------------------\n",
      "[ Epoch98: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36881 ACC:0.84031 Precision:0.86286 Recall:0.81107 AUC:0.84046 F1:0.83617 Time:44.080479\n",
      "------------------------------------------------------\n",
      "[ Epoch99: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36888 ACC:0.83984 Precision:0.86297 Recall:0.80981 AUC:0.83998 F1:0.83554 Time:44.535180\n",
      "------------------------------------------------------\n",
      "[ Epoch100: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36885 ACC:0.83989 Precision:0.86323 Recall:0.80959 AUC:0.84004 F1:0.83555 Time:44.313441\n",
      "------------------------------------------------------\n",
      "best_acc 0.8454106280193237\n",
      "best_precision 0.8600791382721478\n",
      "best_recall 0.8600791382721478\n",
      "best_f1 0.8431203534101929\n",
      "best_auc 0.8455008863429327\n"
     ]
    }
   ],
   "source": [
    "# 进行训练\n",
    "best_acc, best_precision, best_recall, best_f1, best_auc = training(epoch, lr, train_loader, val_loader, APJFFF_model, device, model_name, model_dir)\n",
    "\n",
    "# 输出结果（验证集）\n",
    "print('best_acc',best_acc)\n",
    "print('best_precision',best_precision)\n",
    "print('best_recall',best_precision)\n",
    "print('best_f1',best_f1)\n",
    "print('best_auc',best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a4cc60-f49b-47bf-b856-749ae71b9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "    pred_label = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (jobs, users, entities, labels) in enumerate(test_loader):\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            entities = entities.to(torch.float32)\n",
    "            entities = entities.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(jobs, users, entities)\n",
    "\n",
    "            pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "\n",
    "        test_acc = accuracy_score(y_label, pred_label)\n",
    "        test_precision = precision_score(y_label, pred_label)\n",
    "        test_recall = recall_score(y_label, pred_label)\n",
    "        test_auc = roc_auc_score(y_label, pred_label)\n",
    "        test_f1 = f1_score(y_label, pred_label)\n",
    "    return test_acc, test_auc, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ed8cc2b-4e8c-4c73-b930-465fecf056e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc 0.8344747040399214\n",
      "test_precision 0.8359675352413498\n",
      "test_recall 0.8359675352413498\n",
      "test_f1 0.8339192500266326\n",
      "test_auc 0.8344723653665331\n"
     ]
    }
   ],
   "source": [
    "# 输出结果(测试集)\n",
    "test_acc, test_auc, test_precision, test_recall, test_f1 = testing(\n",
    "    torch.load('APJFFF_att.model'), test_loader)\n",
    "print('test_acc', test_acc)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_precision)\n",
    "print('test_f1', test_f1)\n",
    "print('test_auc', test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
