{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fb28321-238b-4db2-b4ef-c36dc08ec153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score\n",
    "import datetime\n",
    "import time\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForMaskedLM\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import word2vec, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad166e6-d167-471d-914e-2959eb3db314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 设置随机数种子\n",
    "setup_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ddf018-af0f-46bf-b4e3-aff355a1a933",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('/root/autodl-fs/dataset/dataset_user_job_all_test1.csv', dtype = {'UserID': 'str', 'JobID': 'str','label': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7194e91-187f-4cf1-91cf-a886a5d46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_job = dataset['skill_entity_en_job'].values\n",
    "skill_user = dataset['skill_entity_en_user'].values\n",
    "\n",
    "skill_job_emb = []\n",
    "for skills in skill_job:\n",
    "    skill_job_emb.append(skills.split(','))\n",
    "dataset['skill_job'] = skill_job_emb\n",
    "\n",
    "skill_user_emb = []\n",
    "for skills in skill_user:\n",
    "    skill_user_emb.append(skills.split(','))\n",
    "dataset['skill_user'] = skill_user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeddd2d0-e037-4adf-a7de-1a4a0a0061e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_array = np.concatenate((skill_job_emb,skill_user_emb),axis=0)\n",
    "# w2v_model = word2vec.Word2Vec(text_array, size=100, window=5, min_count=2, workers=8, iter=10, sg=1)\n",
    "w2v_model = word2vec.Word2Vec(text_array, size=8, window=5, min_count=2, workers=8, iter=10, sg=1)\n",
    "w2v_model.save('word2vec_all.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6261cdc7-f7b9-438d-8726-0eb1084b9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('word2vec_all.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffe314b-5cf7-4406-82e8-684b22e85778",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_present_list = list(w2v_model.wv.index2word)\n",
    "dataset['skill_job'] = dataset['skill_job'].apply(lambda x:[i for i in x if i in word_present_list])\n",
    "dataset['skill_user'] = dataset['skill_user'].apply(lambda x:[i for i in x if i in word_present_list])\n",
    "# 先用列平均值，后面看情况再改\n",
    "dataset['skill_job'] = dataset['skill_job'].apply(lambda x: np.mean([np.array(w2v_model.wv[i]).reshape(1,8) for i in x], axis=0))\n",
    "dataset['skill_user'] = dataset['skill_user'].apply(lambda x: np.mean([np.array(w2v_model.wv[i]).reshape(1,8) for i in x], axis=0))\n",
    "\n",
    "skill_user_1 = []\n",
    "skill_user_2 = []\n",
    "skill_user_3 = []\n",
    "skill_user_4 = []\n",
    "skill_user_5 = []\n",
    "skill_user_6 = []\n",
    "skill_user_7 = []\n",
    "skill_user_8 = []\n",
    "\n",
    "skill_job_1 = []\n",
    "skill_job_2 = []\n",
    "skill_job_3 = []\n",
    "skill_job_4 = []\n",
    "skill_job_5 = []\n",
    "skill_job_6 = []\n",
    "skill_job_7 = []\n",
    "skill_job_8 = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        skill_job_embedding = dataset.loc[i, 'skill_job'][0].tolist()\n",
    "        skill_job_1.append(skill_job_embedding[0])\n",
    "        skill_job_2.append(skill_job_embedding[1])\n",
    "        skill_job_3.append(skill_job_embedding[2])\n",
    "        skill_job_4.append(skill_job_embedding[3])\n",
    "        skill_job_5.append(skill_job_embedding[4])\n",
    "        skill_job_6.append(skill_job_embedding[5])\n",
    "        skill_job_7.append(skill_job_embedding[6])\n",
    "        skill_job_8.append(skill_job_embedding[7])\n",
    "    except:\n",
    "        skill_job_1.append(0)\n",
    "        skill_job_2.append(0)\n",
    "        skill_job_3.append(0)\n",
    "        skill_job_4.append(0)\n",
    "        skill_job_5.append(0)\n",
    "        skill_job_6.append(0)\n",
    "        skill_job_7.append(0)\n",
    "        skill_job_8.append(0)\n",
    "    skill_user_embedding = dataset.loc[i, 'skill_user'][0].tolist()\n",
    "    skill_user_1.append(skill_user_embedding[0])\n",
    "    skill_user_2.append(skill_user_embedding[1])\n",
    "    skill_user_3.append(skill_user_embedding[2])\n",
    "    skill_user_4.append(skill_user_embedding[3])\n",
    "    skill_user_5.append(skill_user_embedding[4])\n",
    "    skill_user_6.append(skill_user_embedding[5])\n",
    "    skill_user_7.append(skill_user_embedding[6])\n",
    "    skill_user_8.append(skill_user_embedding[7])\n",
    "\n",
    "\n",
    "dataset['skill_user_1'] = skill_user_1\n",
    "dataset['skill_user_2'] = skill_user_2\n",
    "dataset['skill_user_3'] = skill_user_3\n",
    "dataset['skill_user_4'] = skill_user_4\n",
    "dataset['skill_user_5'] = skill_user_5\n",
    "dataset['skill_user_6'] = skill_user_6\n",
    "dataset['skill_user_7'] = skill_user_7\n",
    "dataset['skill_user_8'] = skill_user_8\n",
    "\n",
    "dataset['skill_job_1'] = skill_job_1\n",
    "dataset['skill_job_2'] = skill_job_2\n",
    "dataset['skill_job_3'] = skill_job_3\n",
    "dataset['skill_job_4'] = skill_job_4\n",
    "dataset['skill_job_5'] = skill_job_5\n",
    "dataset['skill_job_6'] = skill_job_6\n",
    "dataset['skill_job_7'] = skill_job_7\n",
    "dataset['skill_job_8'] = skill_job_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb9726d-bf7d-4161-987b-2f23706b9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稠密特征\n",
    "dense_feas = ['岗位薪资下限(K)','岗位薪资上限(K)','work_length','总分','岗位招聘人数']\n",
    "\n",
    "# 文本特征\n",
    "vec_feas = ['skill_job_1','skill_job_2','skill_job_3','skill_job_4',\n",
    "            'skill_job_5','skill_job_6','skill_job_7','skill_job_8',\n",
    "            'skill_user_1','skill_user_2','skill_user_3','skill_user_4',\n",
    "            'skill_user_5','skill_user_6','skill_user_7','skill_user_8']\n",
    "\n",
    "text_feas = ['岗位名称', '岗位描述', 'experience']\n",
    "\n",
    "# 稀疏特征\n",
    "## TODO userid jobid应该放进去吗\n",
    "sparse_feas_user = ['UserID','性别','专业']\n",
    "sparse_feas_job = ['JobID','企业融资阶段','企业人员规模','企业休息时间','企业加班情况','岗位一级类别','岗位三级类别','岗位工作经验','岗位招聘类型'] # '岗位二级类别'\n",
    "sparse_feas_match = ['match_degree','match_loc_job','match_loc_corp']\n",
    "sparse_feas = sparse_feas_user + sparse_feas_job + sparse_feas_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a216ad15-c327-481a-9221-17acdd5f5b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseFeature(feat, feat_num, embed_dim=8):\n",
    "    # if len(dataset[feat].unique()) < embed_dim:\n",
    "    #     embed_dim = len(dataset[feat].unique())\n",
    "    return {'feat':feat, 'feat_num':feat_num, 'embed_dim':embed_dim}\n",
    "\n",
    "def denseFeature(feat):\n",
    "    return {'feat':feat}\n",
    "\n",
    "def vecFeature(feat):\n",
    "    return{'feat':feat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c8c94ee-5164-41c9-995a-1be525d8fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "feature_columns = [[denseFeature(feat) for feat in dense_feas]] +[[sparseFeature(feat, len(dataset[feat].unique()), embed_dim=embed_dim) for feat in sparse_feas]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cc9ccd-ffcc-4ec1-8d53-c51d7adb9028",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FM(nn.Module):\n",
    "    def __init__(self, latent_dim, fea_num):\n",
    "        \"\"\"\n",
    "        latent_dim:各个离散特征隐向量的维度\n",
    "        fea_num:特征个数\n",
    "        \"\"\"\n",
    "        super(FM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # print('fea_num',fea_num)   #82\n",
    "        #定义三个矩阵，一个是全局偏置，一个是一阶权重矩阵，一个是二阶交叉矩阵\n",
    "        self.w0 = nn.Parameter(torch.zeros([1,]))\n",
    "        self.w1 = nn.Parameter(torch.rand([fea_num, 1]))\n",
    "        self.w2 = nn.Parameter(torch.rand([fea_num, latent_dim]))\n",
    "    def forward(self, x):\n",
    "        #x的维度是(batch_size, fea_num)\n",
    "        #一阶交叉\n",
    "        # x=x[:,:82]   #[32,222]\n",
    "        # print(\"x\",x.shape)\n",
    "        first_order = self.w0 + torch.mm(x, self.w1) #(batch_size, 1)\n",
    "        #二阶交叉\n",
    "        second_order = 1/2 * torch.sum(torch.pow(torch.mm(x, self.w2), 2) - torch.mm(torch.pow(x, 2), torch.pow(self.w2, 2)), dim=1, keepdim=True)\n",
    "        return first_order + second_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5515e3-e33e-4d45-8aec-d01eeec52475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dnn(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout=0.):\n",
    "        \"\"\"\n",
    "        hidden_units:列表，每个元素表示每一层的神经单元个数，比如[256,128,64]两层网络，第一个维度是输入维度\n",
    "        \"\"\"\n",
    "        super(Dnn, self).__init__()\n",
    "        self.dnn_network = nn.ModuleList([nn.Linear(layer[0], layer[1]) for layer in list(zip(hidden_units[:-1], hidden_units[1:]))])\n",
    "        #layer[0]: (128,64)   layer[1]:(64,32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        for linear in self.dnn_network:\n",
    "            # print(\"linear\",linear)\n",
    "            # print(\"x1\",x.shape)  # [32,102]\n",
    "            x = linear(x)\n",
    "            # print(\"x2\",x.shape)\n",
    "            x = F.relu(x)\n",
    "            # print(\"x2\", x.shape)\n",
    "        x = self.dropout(x)\n",
    "        # print(x,x.shape)   [32,32]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2200344f-0ead-4206-85b2-700ea06d0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, feature_columns, hidden_units, dnn_dropout=0.):\n",
    "        \"\"\"\n",
    "        feature_columns:特征信息\n",
    "        hidden_units:dnn的隐藏单元个数\n",
    "        dnn_dropout:失活率\n",
    "        \"\"\"\n",
    "        super(DeepFM, self).__init__()\n",
    "        self.dense_feature_cols, self.sparse_feature_cols = feature_columns\n",
    "        print(self.sparse_feature_cols)\n",
    "\n",
    "        # embedding\n",
    "        self.embed_layers = nn.ModuleDict({\n",
    "            'embed_' + str(i): nn.Embedding(num_embeddings=feat['feat_num'], embedding_dim=feat['embed_dim']) for\n",
    "            i, feat in enumerate(self.sparse_feature_cols)    #len=26\n",
    "        })\n",
    "\n",
    "        self.fea_num = len(self.dense_feature_cols)\n",
    "        for one in self.sparse_feature_cols:\n",
    "            self.fea_num += one[\"embed_dim\"]\n",
    "        self.fea_num += len(vec_feas)\n",
    "        hidden_units.insert(0, self.fea_num)  #在hidden_units的最前面插入self.fea_num\n",
    "\n",
    "        self.fm = FM(self.sparse_feature_cols[0]['embed_dim'], self.fea_num)\n",
    "        self.dnn_network = Dnn(hidden_units, dnn_dropout)\n",
    "        self.nn_final_linear = nn.Linear(hidden_units[-1], 1)  #[32,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        dense_inputs, sparse_inputs= x[:, :len(self.dense_feature_cols)], x[:, len(self.dense_feature_cols):len(self.dense_feature_cols) + 15]\n",
    "        vec_inputs=x[:,len(self.dense_feature_cols) + 15:]\n",
    "        sparse_inputs = sparse_inputs.long()     #将数字或字符串转换成长整型\n",
    "        sparse_embeds = [self.embed_layers['embed_' + str(i)](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]     #for i in range(10)   0-9\n",
    "        sparse_embeds = torch.cat(sparse_embeds, dim=-1)\n",
    "\n",
    "        # 把离散特征、连续特征、文本向量 拼接作为FM和DNN的输入\n",
    "        x = torch.cat([sparse_embeds, dense_inputs, vec_inputs], dim=-1)\n",
    "        # Wide\n",
    "        wide_outputs = self.fm(x)\n",
    "        # deep\n",
    "        deep_outputs = self.nn_final_linear(self.dnn_network(x))\n",
    "\n",
    "        # 模型的最后输出\n",
    "        outputs = torch.add(wide_outputs, deep_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f00e7c-2f5b-4117-8488-1618f9959704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobUserDataset(data.Dataset):\n",
    "    '''\n",
    "    Expected data shape like:(data_num, data_len)\n",
    "    '''\n",
    "    def __init__(self, job, user, deepfm, label):\n",
    "        self.job = job\n",
    "        self.user = user\n",
    "        self.deepfm = deepfm\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.job[idx], self.user[idx], self.deepfm[idx]\n",
    "        return self.job[idx], self.user[idx], self.deepfm[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "219e9f1f-2141-42f1-809d-4932c09ae0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1905f706-7607-40af-afc1-0af87fca0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim, 1, bias=False),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (N, L, D)\n",
    "        a = self.attn(x)        # (N, L, 1)\n",
    "        x = (x * a).sum(dim=1)  # (N, D)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b32a06d0-77e2-4d5e-826f-9a7938eddcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_layer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attn = SelfAttentionEncoder(dim) # * 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        g = self.attn(x)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2a7ae0c-28ca-41d5-bfce-7dc3aa9fdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAttentionEncoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dim, dim, bias=False)\n",
    "        self.U = nn.Linear(dim, dim, bias=False)\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim, 1, bias=False),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        # (N, L, D), (N, S1, D)\n",
    "        s = s.permute(1, 0, 2)  # (S2, N, D)\n",
    "        # calculate co-attention score\n",
    "        y = torch.cat([self.attn(self.W(x.permute(1, 0, 2)) + self.U( _.expand(x.shape[1], _.shape[0], _.shape[1]) ) ).permute(2, 0, 1) for _ in s ]).permute(2, 0, 1)\n",
    "        # (N, D) -> (L, N, D) -> (L, N, 1) -- softmax as L --> (L, N, 1) -> (1, L, N) -> (S2, L, N) -> (N, S2, L)\n",
    "        sr = torch.cat([torch.mm(y[i], _).unsqueeze(0) for i, _ in enumerate(x)])   # (N, S2, D)\n",
    "        sr = torch.sum(sr, dim=1)  # (N, D)\n",
    "        return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f8f634b-7d6f-40fb-b43b-e41405d7207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAttention_layer(nn.Module):\n",
    "    def __init__(self, dim, hd_size):\n",
    "        super().__init__()\n",
    "        self.co_attn = CoAttentionEncoder(dim)\n",
    "        self.biLSTM = nn.LSTM(\n",
    "            input_size=dim,\n",
    "            hidden_size=hd_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.self_attn = SelfAttentionEncoder(dim) # * 2\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        # x: self\n",
    "        # s: other\n",
    "        # (N, S1, D), (N, S2, D)\n",
    "        s = s.unsqueeze(2)\n",
    "        s = s.permute(1, 0, 2, 3)  # (S2, N, L, D)\n",
    "        sr = torch.cat([self.co_attn(x, _).unsqueeze(0) for _ in s])   # (S1, N, D)\n",
    "        c = sr.permute(1, 0, 2)     # (N, S1, D)\n",
    "        # c = self.biLSTM(u)[0]       # (N, S1, D)\n",
    "        g = self.self_attn(c)       # (N, D)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2102e626-3641-4144-a640-988da5e39eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APJFFF_text(nn.Module):\n",
    "    def __init__(self, lstm_dim, lstm_hd_size, num_lstm_layers, dropout): #, dropout\n",
    "        super(APJFFF_text, self).__init__()\n",
    "        '''\n",
    "        APJFFF setting\n",
    "        '''\n",
    "        self.user_biLSTM = nn.LSTM(\n",
    "            input_size=lstm_dim,\n",
    "            hidden_size=lstm_hd_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.job_biLSTM = nn.LSTM(\n",
    "            input_size=lstm_dim,\n",
    "            hidden_size=lstm_hd_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "\n",
    "        self.job_layer_1 = Attention_layer(lstm_hd_size * 2) #, lstm_hd_size\n",
    "        self.job_layer_2 = CoAttention_layer(lstm_hd_size * 2, lstm_hd_size)\n",
    "\n",
    "        self.user_layer_1 = Attention_layer(lstm_hd_size * 2 ) #, lstm_hd_size\n",
    "        self.user_layer_2 = CoAttention_layer(lstm_hd_size * 2 , lstm_hd_size)\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_size=lstm_hd_size * 2 * 4,\n",
    "            output_size=1,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.liner_layer = nn.Linear(lstm_hd_size * 2 * 8, 1)\n",
    "        # self.pool_layer = nn.AdaptiveAvgPool2d((1))\n",
    "\n",
    "    def forward(self, job, user):\n",
    "        # print('usersize',user.size()) # torch.Size([128, 10, 768])\n",
    "\n",
    "        # LSTM part\n",
    "        user_vecs = self.user_biLSTM(user)[0] #.unsqueeze(2)\n",
    "        # print('usersize', user_vecs.size()) # torch.Size([128, 1, 10, 128])\n",
    "        job_vecs = self.job_biLSTM(job)[0] #.unsqueeze(2)\n",
    "\n",
    "        # attention part: bilstm + attention\n",
    "        # gj = self.job_layer_1(job_vecs)\n",
    "        # gr = self.user_layer_1(user_vecs)\n",
    "\n",
    "        # coAttention part\n",
    "        gjj = self.job_layer_2(job_vecs,user_vecs)\n",
    "        grr = self.user_layer_2(user_vecs, job_vecs)\n",
    "\n",
    "        # concat the vectors\n",
    "        # x = torch.cat([gjj, grr, gjj - grr, gjj * grr, gj , gr, gj - gr, gj * gr], axis=1)\n",
    "        x = torch.cat([gjj, grr, gjj - grr, gjj * grr], axis=1)\n",
    "        # print('x_size1:', x.size()) # x_size1: torch.Size([128, 1024])\n",
    "\n",
    "        # fully connected layer\n",
    "        # x = self.liner_layer(x)\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbf39f9f-2467-46ea-a040-bdcb6000c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APJFF_connect(nn.Module):\n",
    "    def __init__(self, lstm_dim, lstm_hd_size, num_lstm_layers, vec_size_2, dropout, feature_columns, hidden_units, dnn_dropout):\n",
    "        super(APJFF_connect, self).__init__()\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_size=1, # 2049\n",
    "            output_size=1,\n",
    "            dropout= 0.7 #dropout\n",
    "        )\n",
    "        self.atten_layer = Attention_layer(1)\n",
    "        self.linear_layer = nn.Linear(2,1)\n",
    "        self.text_layer = APJFFF_text(lstm_dim, lstm_hd_size, num_lstm_layers, dropout)\n",
    "        self.entity_layer = DeepFM(feature_columns, hidden_units, dnn_dropout)\n",
    "\n",
    "    def forward(self, job, user, x):\n",
    "        text_vec = self.text_layer(job, user)\n",
    "        entity_vec = self.entity_layer(x)\n",
    "\n",
    "        # attention\n",
    "        vec = torch.stack([text_vec, entity_vec]) # (N, S, D) -> (N, D)\n",
    "        vec = vec.permute(1, 0, 2) # 2, 1, 0, 3\n",
    "        vec_att = self.atten_layer(vec)\n",
    "        x = self.mlp(vec_att)\n",
    "        x = x.squeeze(1)\n",
    "        # x = F.sigmoid(vec_att).squeeze()\n",
    "\n",
    "        # no attention\n",
    "        # vec = entity_vec\n",
    "        # vec = torch.cat([text_vec,entity_vec], axis=1)\n",
    "        # print('vecsize',vec.size())\n",
    "        # x = self.mlp(vec).squeeze(1)\n",
    "        # x = F.sigmoid(vec).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d9a7d5-fac5-41ed-920e-9f789daecbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(n_epoch, lr, train, valid, model, device, model_name, model_dir=\"./\"):\n",
    "    # summary model parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\nstart training, total parameter:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch, eta_min=0, last_epoch=-1)\n",
    "    # total_loss, total_acc = 0, 0\n",
    "    best_acc, best_precision, best_recall, best_f1, best_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        pred_label = []\n",
    "        y_label = []\n",
    "        # training\n",
    "        for i, (jobs, users, entities, labels) in enumerate(train):\n",
    "\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            entities = entities.to(torch.float32)\n",
    "            entities = entities.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO 是否考虑模型用多个优化器？\n",
    "            optimizer.zero_grad() # 将所有模型参数的梯度置为0\n",
    "            # model.zero_grad() # 除所有可训练的torch.Tensor的梯度\n",
    "            outputs = model(jobs, users, entities)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_label.extend([0 if i<0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "        train_losses = total_loss/t_batch\n",
    "        train_acc = accuracy_score(y_label, pred_label)\n",
    "        train_precision = precision_score(y_label, pred_label)\n",
    "        train_recall = recall_score(y_label, pred_label)\n",
    "        train_auc = roc_auc_score(y_label, pred_label)\n",
    "        train_f1 = f1_score(y_label, pred_label)\n",
    "        print('[ Epoch{}: {}/{}] '.format(epoch+1, i+1, t_batch))\n",
    "        # print('\\nTrain | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(train_losses,train_acc,train_precision, train_recall,train_auc,train_f1, time.time()-start_time))\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # pred_score = []\n",
    "            pred_label = []\n",
    "            y_label = []\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (jobs, users, entities, labels) in enumerate(valid):\n",
    "                # 放GPU上运行\n",
    "                jobs = jobs.to(torch.float32)\n",
    "                jobs = jobs.to(device)\n",
    "\n",
    "                users = users.to(torch.float32)\n",
    "                users = users.to(device)\n",
    "\n",
    "                entities = entities.to(torch.float32)\n",
    "                entities = entities.to(device)\n",
    "\n",
    "                labels = labels.to(torch.float32)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(jobs, users, entities)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                '''\n",
    "                存一下预测score\n",
    "                '''\n",
    "                # pred_score.extend([j for j in list(outputs.cpu().detach().numpy())])\n",
    "                pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "                y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "            val_losses = total_loss/v_batch\n",
    "            val_acc = accuracy_score(y_label, pred_label)\n",
    "            val_precision = precision_score(y_label, pred_label)\n",
    "            val_recall = recall_score(y_label, pred_label)\n",
    "            val_auc = roc_auc_score(y_label, pred_label)\n",
    "            val_f1 = f1_score(y_label, pred_label)\n",
    "            print('\\nVal | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(val_losses,val_acc,val_precision, val_recall,val_auc,val_f1, time.time()-start_time))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_precision = val_precision\n",
    "                best_recall = val_recall\n",
    "                best_f1 = val_f1\n",
    "                best_auc = val_auc\n",
    "                torch.save(model, \"{}/{}.model\".format(model_dir, model_name))\n",
    "                print('save model with acc: {:.3f}, recall: {:.3f}, auc: {:.3f}'.format(best_acc,best_recall,best_auc))\n",
    "        print('------------------------------------------------------')\n",
    "        lr_scheduler.step()\n",
    "        model.train()\n",
    "    return best_acc, best_precision, best_recall, best_f1, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc6f5931-a2ac-44a7-9d73-d83c8064217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'feat': 'UserID', 'feat_num': 7921, 'embed_dim': 8}, {'feat': '性别', 'feat_num': 2, 'embed_dim': 8}, {'feat': '专业', 'feat_num': 935, 'embed_dim': 8}, {'feat': 'JobID', 'feat_num': 31647, 'embed_dim': 8}, {'feat': '企业融资阶段', 'feat_num': 9, 'embed_dim': 8}, {'feat': '企业人员规模', 'feat_num': 7, 'embed_dim': 8}, {'feat': '企业休息时间', 'feat_num': 4, 'embed_dim': 8}, {'feat': '企业加班情况', 'feat_num': 3, 'embed_dim': 8}, {'feat': '岗位一级类别', 'feat_num': 21, 'embed_dim': 8}, {'feat': '岗位三级类别', 'feat_num': 530, 'embed_dim': 8}, {'feat': '岗位工作经验', 'feat_num': 5, 'embed_dim': 8}, {'feat': '岗位招聘类型', 'feat_num': 4, 'embed_dim': 8}, {'feat': 'match_degree', 'feat_num': 2, 'embed_dim': 8}, {'feat': 'match_loc_job', 'feat_num': 2, 'embed_dim': 8}, {'feat': 'match_loc_corp', 'feat_num': 2, 'embed_dim': 8}]\n"
     ]
    }
   ],
   "source": [
    "# 设置模型参数\n",
    "hidden_units = [128, 64, 32]\n",
    "dnn_dropout = 0.7\n",
    "\n",
    "\n",
    "lstm_dim = 768\n",
    "lstm_hd_size = 128\n",
    "num_lstm_layers = 1\n",
    "dropout = 0.7\n",
    "vec_size_2 = 32\n",
    "\n",
    "# 训练参数\n",
    "epoch = 100\n",
    "lr = 0.005\n",
    "# batch_size = 4012\n",
    "batch_size = 32\n",
    "model_dir = './' # change with your model save path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"APJFFF_att\"\n",
    "# 定义模型\n",
    "APJFFF_model = APJFF_connect(lstm_dim, lstm_hd_size, num_lstm_layers, vec_size_2, dropout, feature_columns, hidden_units, dnn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0086944-9fe6-40e8-abfc-56f889f02c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(\"/root/autodl-fs/apjfff_dataset/train_3.dataset\")\n",
    "val_dataset = torch.load(\"/root/autodl-fs/apjfff_dataset/val_3.dataset\")\n",
    "test_dataset = torch.load(\"/root/autodl-fs/apjfff_dataset/test_3.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3f08e0c-12c4-4a6a-a019-7099b7edb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader导入\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8481161e-6e4a-40fb-b003-1d8585310ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, total parameter:4567731, trainable:4567731\n",
      "\n",
      "[ Epoch1: 1766/1766] \n",
      "\n",
      "Val | Loss:0.68201 ACC:0.55582 Precision:0.71518 Recall:0.19262 AUC:0.55758 F1:0.30350 Time:237.252927\n",
      "save model with acc: 0.556, recall: 0.193, auc: 0.558\n",
      "------------------------------------------------------\n",
      "[ Epoch2: 1766/1766] \n",
      "\n",
      "Val | Loss:0.65627 ACC:0.65547 Precision:0.71513 Recall:0.52230 AUC:0.65611 F1:0.60369 Time:240.722205\n",
      "save model with acc: 0.655, recall: 0.522, auc: 0.656\n",
      "------------------------------------------------------\n",
      "[ Epoch3: 1766/1766] \n",
      "\n",
      "Val | Loss:0.62379 ACC:0.67426 Precision:0.91746 Recall:0.38641 AUC:0.67566 F1:0.54379 Time:240.512936\n",
      "save model with acc: 0.674, recall: 0.386, auc: 0.676\n",
      "------------------------------------------------------\n",
      "[ Epoch4: 1766/1766] \n",
      "\n",
      "Val | Loss:0.56073 ACC:0.74221 Precision:0.86665 Recall:0.57544 AUC:0.74302 F1:0.69164 Time:240.219293\n",
      "save model with acc: 0.742, recall: 0.575, auc: 0.743\n",
      "------------------------------------------------------\n",
      "[ Epoch5: 1766/1766] \n",
      "\n",
      "Val | Loss:0.55787 ACC:0.76891 Precision:0.86069 Recall:0.64434 AUC:0.76952 F1:0.73696 Time:240.695987\n",
      "save model with acc: 0.769, recall: 0.644, auc: 0.770\n",
      "------------------------------------------------------\n",
      "[ Epoch6: 1766/1766] \n",
      "\n",
      "Val | Loss:0.51847 ACC:0.77645 Precision:0.75374 Recall:0.82439 AUC:0.77622 F1:0.78748 Time:244.680696\n",
      "save model with acc: 0.776, recall: 0.824, auc: 0.776\n",
      "------------------------------------------------------\n",
      "[ Epoch7: 1766/1766] \n",
      "\n",
      "Val | Loss:0.48694 ACC:0.78479 Precision:0.77202 Recall:0.81118 AUC:0.78466 F1:0.79112 Time:244.709292\n",
      "save model with acc: 0.785, recall: 0.811, auc: 0.785\n",
      "------------------------------------------------------\n",
      "[ Epoch8: 1766/1766] \n",
      "\n",
      "Val | Loss:0.48688 ACC:0.80066 Precision:0.82272 Recall:0.76891 AUC:0.80081 F1:0.79491 Time:247.840014\n",
      "save model with acc: 0.801, recall: 0.769, auc: 0.801\n",
      "------------------------------------------------------\n",
      "[ Epoch9: 1766/1766] \n",
      "\n",
      "Val | Loss:0.45988 ACC:0.80846 Precision:0.81572 Recall:0.79934 AUC:0.80851 F1:0.80745 Time:247.649012\n",
      "save model with acc: 0.808, recall: 0.799, auc: 0.809\n",
      "------------------------------------------------------\n",
      "[ Epoch10: 1766/1766] \n",
      "\n",
      "Val | Loss:0.47808 ACC:0.81122 Precision:0.83768 Recall:0.77430 AUC:0.81140 F1:0.80474 Time:244.359924\n",
      "save model with acc: 0.811, recall: 0.774, auc: 0.811\n",
      "------------------------------------------------------\n",
      "[ Epoch11: 1766/1766] \n",
      "\n",
      "Val | Loss:0.45767 ACC:0.81037 Precision:0.79478 Recall:0.83929 AUC:0.81023 F1:0.81643 Time:243.976772\n",
      "------------------------------------------------------\n",
      "[ Epoch12: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42929 ACC:0.80756 Precision:0.78369 Recall:0.85218 AUC:0.80734 F1:0.81650 Time:249.371567\n",
      "------------------------------------------------------\n",
      "[ Epoch13: 1766/1766] \n",
      "\n",
      "Val | Loss:0.45224 ACC:0.81239 Precision:0.79799 Recall:0.83897 AUC:0.81226 F1:0.81797 Time:245.718419\n",
      "save model with acc: 0.812, recall: 0.839, auc: 0.812\n",
      "------------------------------------------------------\n",
      "[ Epoch14: 1766/1766] \n",
      "\n",
      "Val | Loss:0.43410 ACC:0.82009 Precision:0.81883 Recall:0.82428 AUC:0.82007 F1:0.82155 Time:242.200438\n",
      "save model with acc: 0.820, recall: 0.824, auc: 0.820\n",
      "------------------------------------------------------\n",
      "[ Epoch15: 1766/1766] \n",
      "\n",
      "Val | Loss:0.43096 ACC:0.82253 Precision:0.81406 Recall:0.83823 AUC:0.82245 F1:0.82597 Time:246.527946\n",
      "save model with acc: 0.823, recall: 0.838, auc: 0.822\n",
      "------------------------------------------------------\n",
      "[ Epoch16: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44326 ACC:0.82157 Precision:0.80895 Recall:0.84425 AUC:0.82146 F1:0.82622 Time:247.859174\n",
      "------------------------------------------------------\n",
      "[ Epoch17: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42403 ACC:0.82540 Precision:0.81115 Recall:0.85049 AUC:0.82528 F1:0.83035 Time:247.247244\n",
      "save model with acc: 0.825, recall: 0.850, auc: 0.825\n",
      "------------------------------------------------------\n",
      "[ Epoch18: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42920 ACC:0.81871 Precision:0.79338 Recall:0.86422 AUC:0.81849 F1:0.82729 Time:243.622898\n",
      "------------------------------------------------------\n",
      "[ Epoch19: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42152 ACC:0.82917 Precision:0.82812 Recall:0.83284 AUC:0.82915 F1:0.83047 Time:245.143688\n",
      "save model with acc: 0.829, recall: 0.833, auc: 0.829\n",
      "------------------------------------------------------\n",
      "[ Epoch20: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42408 ACC:0.82370 Precision:0.80792 Recall:0.85154 AUC:0.82356 F1:0.82916 Time:247.195692\n",
      "------------------------------------------------------\n",
      "[ Epoch21: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41351 ACC:0.82816 Precision:0.82805 Recall:0.83041 AUC:0.82815 F1:0.82923 Time:242.425605\n",
      "------------------------------------------------------\n",
      "[ Epoch22: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42582 ACC:0.82837 Precision:0.81755 Recall:0.84753 AUC:0.82828 F1:0.83227 Time:238.875126\n",
      "------------------------------------------------------\n",
      "[ Epoch23: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42309 ACC:0.82810 Precision:0.81142 Recall:0.85704 AUC:0.82796 F1:0.83361 Time:245.385527\n",
      "------------------------------------------------------\n",
      "[ Epoch24: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41320 ACC:0.83336 Precision:0.84476 Recall:0.81879 AUC:0.83343 F1:0.83157 Time:244.977120\n",
      "save model with acc: 0.833, recall: 0.819, auc: 0.833\n",
      "------------------------------------------------------\n",
      "[ Epoch25: 1766/1766] \n",
      "\n",
      "Val | Loss:0.43101 ACC:0.83161 Precision:0.84678 Recall:0.81171 AUC:0.83170 F1:0.82887 Time:244.970486\n",
      "------------------------------------------------------\n",
      "[ Epoch26: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42687 ACC:0.83118 Precision:0.86115 Recall:0.79163 AUC:0.83138 F1:0.82493 Time:244.019004\n",
      "------------------------------------------------------\n",
      "[ Epoch27: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40706 ACC:0.82858 Precision:0.88445 Recall:0.75782 AUC:0.82893 F1:0.81625 Time:243.229075\n",
      "------------------------------------------------------\n",
      "[ Epoch28: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40138 ACC:0.83607 Precision:0.83922 Recall:0.83337 AUC:0.83608 F1:0.83628 Time:244.549998\n",
      "save model with acc: 0.836, recall: 0.833, auc: 0.836\n",
      "------------------------------------------------------\n",
      "[ Epoch29: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44425 ACC:0.83193 Precision:0.81795 Recall:0.85598 AUC:0.83181 F1:0.83653 Time:248.956522\n",
      "------------------------------------------------------\n",
      "[ Epoch30: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39280 ACC:0.83352 Precision:0.83610 Recall:0.83168 AUC:0.83353 F1:0.83388 Time:247.903612\n",
      "------------------------------------------------------\n",
      "[ Epoch31: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40381 ACC:0.83373 Precision:0.85295 Recall:0.80843 AUC:0.83385 F1:0.83010 Time:246.501771\n",
      "------------------------------------------------------\n",
      "[ Epoch32: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40577 ACC:0.83240 Precision:0.85333 Recall:0.80473 AUC:0.83254 F1:0.82832 Time:248.989683\n",
      "------------------------------------------------------\n",
      "[ Epoch33: 1766/1766] \n",
      "\n",
      "Val | Loss:0.44482 ACC:0.82964 Precision:0.81423 Recall:0.85630 AUC:0.82951 F1:0.83473 Time:249.133867\n",
      "------------------------------------------------------\n",
      "[ Epoch34: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40461 ACC:0.83490 Precision:0.84563 Recall:0.82132 AUC:0.83497 F1:0.83330 Time:244.195672\n",
      "------------------------------------------------------\n",
      "[ Epoch35: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40950 ACC:0.83718 Precision:0.85121 Recall:0.81910 AUC:0.83727 F1:0.83485 Time:240.862209\n",
      "save model with acc: 0.837, recall: 0.819, auc: 0.837\n",
      "------------------------------------------------------\n",
      "[ Epoch36: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40727 ACC:0.83458 Precision:0.85495 Recall:0.80780 AUC:0.83471 F1:0.83071 Time:243.977404\n",
      "------------------------------------------------------\n",
      "[ Epoch37: 1766/1766] \n",
      "\n",
      "Val | Loss:0.42575 ACC:0.83453 Precision:0.84947 Recall:0.81509 AUC:0.83462 F1:0.83192 Time:240.178431\n",
      "------------------------------------------------------\n",
      "[ Epoch38: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40268 ACC:0.83580 Precision:0.84264 Recall:0.82777 AUC:0.83584 F1:0.83514 Time:243.425459\n",
      "------------------------------------------------------\n",
      "[ Epoch39: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41501 ACC:0.83416 Precision:0.84828 Recall:0.81583 AUC:0.83425 F1:0.83174 Time:243.967840\n",
      "------------------------------------------------------\n",
      "[ Epoch40: 1766/1766] \n",
      "\n",
      "Val | Loss:0.41276 ACC:0.83214 Precision:0.85349 Recall:0.80389 AUC:0.83228 F1:0.82795 Time:242.995216\n",
      "------------------------------------------------------\n",
      "[ Epoch41: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40172 ACC:0.83633 Precision:0.85041 Recall:0.81815 AUC:0.83642 F1:0.83397 Time:240.139715\n",
      "------------------------------------------------------\n",
      "[ Epoch42: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39564 ACC:0.83532 Precision:0.86446 Recall:0.79723 AUC:0.83551 F1:0.82949 Time:246.124131\n",
      "------------------------------------------------------\n",
      "[ Epoch43: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40103 ACC:0.83527 Precision:0.85422 Recall:0.81044 AUC:0.83539 F1:0.83175 Time:245.510896\n",
      "------------------------------------------------------\n",
      "[ Epoch44: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40185 ACC:0.83883 Precision:0.85600 Recall:0.81657 AUC:0.83894 F1:0.83582 Time:244.636990\n",
      "save model with acc: 0.839, recall: 0.817, auc: 0.839\n",
      "------------------------------------------------------\n",
      "[ Epoch45: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39318 ACC:0.83867 Precision:0.85423 Recall:0.81858 AUC:0.83877 F1:0.83602 Time:241.533158\n",
      "------------------------------------------------------\n",
      "[ Epoch46: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40738 ACC:0.83564 Precision:0.87205 Recall:0.78857 AUC:0.83587 F1:0.82821 Time:239.408794\n",
      "------------------------------------------------------\n",
      "[ Epoch47: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39222 ACC:0.83793 Precision:0.84258 Recall:0.83305 AUC:0.83795 F1:0.83779 Time:242.733181\n",
      "------------------------------------------------------\n",
      "[ Epoch48: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40796 ACC:0.83798 Precision:0.85551 Recall:0.81519 AUC:0.83809 F1:0.83487 Time:244.301541\n",
      "------------------------------------------------------\n",
      "[ Epoch49: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39070 ACC:0.83840 Precision:0.85314 Recall:0.81942 AUC:0.83850 F1:0.83594 Time:240.711928\n",
      "------------------------------------------------------\n",
      "[ Epoch50: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39842 ACC:0.83686 Precision:0.84342 Recall:0.82925 AUC:0.83690 F1:0.83627 Time:243.475884\n",
      "------------------------------------------------------\n",
      "[ Epoch51: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38874 ACC:0.83580 Precision:0.86145 Recall:0.80220 AUC:0.83597 F1:0.83077 Time:244.091853\n",
      "------------------------------------------------------\n",
      "[ Epoch52: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39627 ACC:0.83994 Precision:0.85721 Recall:0.81762 AUC:0.84005 F1:0.83695 Time:245.379843\n",
      "save model with acc: 0.840, recall: 0.818, auc: 0.840\n",
      "------------------------------------------------------\n",
      "[ Epoch53: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39154 ACC:0.83893 Precision:0.86693 Recall:0.80262 AUC:0.83911 F1:0.83353 Time:241.944508\n",
      "------------------------------------------------------\n",
      "[ Epoch54: 1766/1766] \n",
      "\n",
      "Val | Loss:0.40902 ACC:0.83745 Precision:0.84583 Recall:0.82724 AUC:0.83750 F1:0.83643 Time:242.069651\n",
      "------------------------------------------------------\n",
      "[ Epoch55: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37674 ACC:0.83920 Precision:0.85199 Recall:0.82291 AUC:0.83928 F1:0.83719 Time:244.659069\n",
      "------------------------------------------------------\n",
      "[ Epoch56: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39579 ACC:0.83984 Precision:0.85466 Recall:0.82079 AUC:0.83993 F1:0.83738 Time:242.829469\n",
      "------------------------------------------------------\n",
      "[ Epoch57: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39115 ACC:0.83957 Precision:0.85861 Recall:0.81488 AUC:0.83969 F1:0.83617 Time:239.839768\n",
      "------------------------------------------------------\n",
      "[ Epoch58: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38184 ACC:0.84021 Precision:0.83769 Recall:0.84584 AUC:0.84018 F1:0.84175 Time:244.483465\n",
      "save model with acc: 0.840, recall: 0.846, auc: 0.840\n",
      "------------------------------------------------------\n",
      "[ Epoch59: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38001 ACC:0.84010 Precision:0.85180 Recall:0.82534 AUC:0.84017 F1:0.83836 Time:242.819055\n",
      "------------------------------------------------------\n",
      "[ Epoch60: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37830 ACC:0.84031 Precision:0.86294 Recall:0.81097 AUC:0.84046 F1:0.83615 Time:239.485230\n",
      "save model with acc: 0.840, recall: 0.811, auc: 0.840\n",
      "------------------------------------------------------\n",
      "[ Epoch61: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38561 ACC:0.83920 Precision:0.86205 Recall:0.80949 AUC:0.83934 F1:0.83494 Time:241.953123\n",
      "------------------------------------------------------\n",
      "[ Epoch62: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38327 ACC:0.84079 Precision:0.84473 Recall:0.83696 AUC:0.84081 F1:0.84083 Time:241.590724\n",
      "save model with acc: 0.841, recall: 0.837, auc: 0.841\n",
      "------------------------------------------------------\n",
      "[ Epoch63: 1766/1766] \n",
      "\n",
      "Val | Loss:0.39022 ACC:0.84069 Precision:0.85321 Recall:0.82481 AUC:0.84076 F1:0.83877 Time:243.457404\n",
      "------------------------------------------------------\n",
      "[ Epoch64: 1766/1766] \n",
      "\n",
      "Val | Loss:0.38622 ACC:0.83989 Precision:0.87075 Recall:0.80008 AUC:0.84008 F1:0.83392 Time:241.616937\n",
      "------------------------------------------------------\n",
      "[ Epoch65: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37586 ACC:0.84223 Precision:0.84554 Recall:0.83929 AUC:0.84224 F1:0.84240 Time:241.865276\n",
      "save model with acc: 0.842, recall: 0.839, auc: 0.842\n",
      "------------------------------------------------------\n",
      "[ Epoch66: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37718 ACC:0.84265 Precision:0.84362 Recall:0.84309 AUC:0.84265 F1:0.84336 Time:239.723312\n",
      "save model with acc: 0.843, recall: 0.843, auc: 0.843\n",
      "------------------------------------------------------\n",
      "[ Epoch67: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37652 ACC:0.84010 Precision:0.84861 Recall:0.82978 AUC:0.84015 F1:0.83909 Time:244.963001\n",
      "------------------------------------------------------\n",
      "[ Epoch68: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37787 ACC:0.84164 Precision:0.86267 Recall:0.81445 AUC:0.84177 F1:0.83787 Time:242.679277\n",
      "------------------------------------------------------\n",
      "[ Epoch69: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37084 ACC:0.84164 Precision:0.84418 Recall:0.83981 AUC:0.84165 F1:0.84199 Time:242.890065\n",
      "------------------------------------------------------\n",
      "[ Epoch70: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36869 ACC:0.84042 Precision:0.85159 Recall:0.82639 AUC:0.84049 F1:0.83880 Time:244.795814\n",
      "------------------------------------------------------\n",
      "[ Epoch71: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37531 ACC:0.84148 Precision:0.86125 Recall:0.81593 AUC:0.84161 F1:0.83798 Time:243.061547\n",
      "------------------------------------------------------\n",
      "[ Epoch72: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37506 ACC:0.84191 Precision:0.85993 Recall:0.81868 AUC:0.84202 F1:0.83880 Time:244.935919\n",
      "------------------------------------------------------\n",
      "[ Epoch73: 1766/1766] \n",
      "\n",
      "Val | Loss:0.36851 ACC:0.84238 Precision:0.84856 Recall:0.83538 AUC:0.84242 F1:0.84191 Time:243.632934\n",
      "------------------------------------------------------\n",
      "[ Epoch74: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37094 ACC:0.84111 Precision:0.86058 Recall:0.81593 AUC:0.84123 F1:0.83766 Time:248.100033\n",
      "------------------------------------------------------\n",
      "[ Epoch75: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37300 ACC:0.84010 Precision:0.85996 Recall:0.81435 AUC:0.84023 F1:0.83654 Time:244.372018\n",
      "------------------------------------------------------\n",
      "[ Epoch76: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37241 ACC:0.83920 Precision:0.85580 Recall:0.81773 AUC:0.83930 F1:0.83633 Time:246.820433\n",
      "------------------------------------------------------\n",
      "[ Epoch77: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37089 ACC:0.83867 Precision:0.84764 Recall:0.82766 AUC:0.83872 F1:0.83753 Time:245.582449\n",
      "------------------------------------------------------\n",
      "[ Epoch78: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37563 ACC:0.83925 Precision:0.85692 Recall:0.81636 AUC:0.83936 F1:0.83615 Time:245.919461\n",
      "------------------------------------------------------\n",
      "[ Epoch79: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37112 ACC:0.84132 Precision:0.85433 Recall:0.82481 AUC:0.84140 F1:0.83931 Time:243.856541\n",
      "------------------------------------------------------\n",
      "[ Epoch80: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37313 ACC:0.84127 Precision:0.86444 Recall:0.81128 AUC:0.84142 F1:0.83702 Time:235.359186\n",
      "------------------------------------------------------\n",
      "[ Epoch81: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37620 ACC:0.83745 Precision:0.86186 Recall:0.80558 AUC:0.83760 F1:0.83277 Time:243.898196\n",
      "------------------------------------------------------\n",
      "[ Epoch82: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37522 ACC:0.84016 Precision:0.86611 Recall:0.80653 AUC:0.84032 F1:0.83526 Time:248.139706\n",
      "------------------------------------------------------\n",
      "[ Epoch83: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37447 ACC:0.83978 Precision:0.86238 Recall:0.81044 AUC:0.83993 F1:0.83560 Time:241.200224\n",
      "------------------------------------------------------\n",
      "[ Epoch84: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37428 ACC:0.83835 Precision:0.86584 Recall:0.80262 AUC:0.83852 F1:0.83303 Time:238.804966\n",
      "------------------------------------------------------\n",
      "[ Epoch85: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37283 ACC:0.83936 Precision:0.86023 Recall:0.81224 AUC:0.83949 F1:0.83554 Time:244.050396\n",
      "------------------------------------------------------\n",
      "[ Epoch86: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37629 ACC:0.83761 Precision:0.86621 Recall:0.80040 AUC:0.83779 F1:0.83201 Time:239.652840\n",
      "------------------------------------------------------\n",
      "[ Epoch87: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37392 ACC:0.83830 Precision:0.85727 Recall:0.81361 AUC:0.83842 F1:0.83487 Time:240.413203\n",
      "------------------------------------------------------\n",
      "[ Epoch88: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37254 ACC:0.83952 Precision:0.85819 Recall:0.81530 AUC:0.83964 F1:0.83620 Time:243.638479\n",
      "------------------------------------------------------\n",
      "[ Epoch89: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37234 ACC:0.83920 Precision:0.85746 Recall:0.81551 AUC:0.83931 F1:0.83596 Time:244.314436\n",
      "------------------------------------------------------\n",
      "[ Epoch90: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37465 ACC:0.83766 Precision:0.86217 Recall:0.80568 AUC:0.83782 F1:0.83297 Time:241.445727\n",
      "------------------------------------------------------\n",
      "[ Epoch91: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37457 ACC:0.83798 Precision:0.86440 Recall:0.80357 AUC:0.83815 F1:0.83288 Time:240.939783\n",
      "------------------------------------------------------\n",
      "[ Epoch92: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37575 ACC:0.83856 Precision:0.86465 Recall:0.80463 AUC:0.83873 F1:0.83356 Time:244.085533\n",
      "------------------------------------------------------\n",
      "[ Epoch93: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37502 ACC:0.83920 Precision:0.86303 Recall:0.80822 AUC:0.83935 F1:0.83472 Time:237.394003\n",
      "------------------------------------------------------\n",
      "[ Epoch94: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37687 ACC:0.83915 Precision:0.86540 Recall:0.80505 AUC:0.83931 F1:0.83414 Time:240.369869\n",
      "------------------------------------------------------\n",
      "[ Epoch95: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37656 ACC:0.83840 Precision:0.86214 Recall:0.80748 AUC:0.83855 F1:0.83392 Time:241.267179\n",
      "------------------------------------------------------\n",
      "[ Epoch96: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37627 ACC:0.83835 Precision:0.86042 Recall:0.80959 AUC:0.83849 F1:0.83423 Time:246.294811\n",
      "------------------------------------------------------\n",
      "[ Epoch97: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37642 ACC:0.83851 Precision:0.86062 Recall:0.80970 AUC:0.83865 F1:0.83439 Time:241.247489\n",
      "------------------------------------------------------\n",
      "[ Epoch98: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37708 ACC:0.83808 Precision:0.86123 Recall:0.80790 AUC:0.83823 F1:0.83371 Time:241.723374\n",
      "------------------------------------------------------\n",
      "[ Epoch99: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37711 ACC:0.83835 Precision:0.86172 Recall:0.80790 AUC:0.83850 F1:0.83394 Time:240.779558\n",
      "------------------------------------------------------\n",
      "[ Epoch100: 1766/1766] \n",
      "\n",
      "Val | Loss:0.37720 ACC:0.83835 Precision:0.86180 Recall:0.80780 AUC:0.83850 F1:0.83392 Time:240.158655\n",
      "------------------------------------------------------\n",
      "best_acc 0.8426501035196687\n",
      "best_precision 0.8436244449143582\n",
      "best_recall 0.8436244449143582\n",
      "best_f1 0.8433569390127893\n",
      "best_auc 0.8426479700284778\n"
     ]
    }
   ],
   "source": [
    "# 进行训练\n",
    "best_acc, best_precision, best_recall, best_f1, best_auc = training(epoch, lr, train_loader, val_loader, APJFFF_model, device, model_name, model_dir)\n",
    "\n",
    "# 输出结果（验证集）\n",
    "print('best_acc',best_acc)\n",
    "print('best_precision',best_precision)\n",
    "print('best_recall',best_precision)\n",
    "print('best_f1',best_f1)\n",
    "print('best_auc',best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33a4cc60-f49b-47bf-b856-749ae71b9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "    pred_label = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (jobs, users, entities, labels) in enumerate(test_loader):\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            entities = entities.to(torch.float32)\n",
    "            entities = entities.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(jobs, users, entities)\n",
    "\n",
    "            pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "\n",
    "        test_acc = accuracy_score(y_label, pred_label)\n",
    "        test_precision = precision_score(y_label, pred_label)\n",
    "        test_recall = recall_score(y_label, pred_label)\n",
    "        test_auc = roc_auc_score(y_label, pred_label)\n",
    "        test_f1 = f1_score(y_label, pred_label)\n",
    "    return test_acc, test_auc, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ed8cc2b-4e8c-4c73-b930-465fecf056e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc 0.8411636672506237\n",
      "test_precision 0.844461142121082\n",
      "test_recall 0.844461142121082\n",
      "test_f1 0.8402221510199723\n",
      "test_auc 0.8411590343471569\n"
     ]
    }
   ],
   "source": [
    "# 输出结果(测试集)\n",
    "test_acc, test_auc, test_precision, test_recall, test_f1 = testing(\n",
    "    torch.load('APJFFF_att.model'), test_loader)\n",
    "print('test_acc', test_acc)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_precision)\n",
    "print('test_f1', test_f1)\n",
    "print('test_auc', test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
