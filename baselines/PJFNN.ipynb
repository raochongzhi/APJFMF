{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import nltk\n",
    "# from nltk import word_tokenize, pos_tag\n",
    "# from nltk.stem import  WordNetLemmatizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset_user_job_all_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stop_words = [line.strip() for line in open('chinese_stopword.txt',encoding='UTF-8').readlines()]\n",
    "\n",
    "def pretreatment(comment):\n",
    "\n",
    "    token_words = jieba.lcut(comment)\n",
    "    token_words = [w for w in token_words if w not in stop_words]\n",
    "    token_words =  pseg.cut(' '.join(token_words))\n",
    "    cleaned_word = []\n",
    "    for word, tag in token_words:\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_word.append(word)\n",
    "    return cleaned_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138238 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.467 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 138238/138238 [02:20<00:00, 986.82it/s] \n"
     ]
    }
   ],
   "source": [
    "segment_job =[]\n",
    "# job_set=pd.read_csv('job_information.csv')\n",
    "for content in tqdm(dataset[\"岗位描述\"].values):\n",
    "#     segment.append(pretreatment(content))\n",
    "    segment_job.append(list(jieba.cut(content)))\n",
    "dataset[\"text_job\"] = segment_job\n",
    "# job_set.to_csv(\"job_set_segment.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138238/138238 [12:24<00:00, 185.69it/s]\n"
     ]
    }
   ],
   "source": [
    "segment_user = []\n",
    "# job_set=pd.read_csv('job_information.csv')\n",
    "for content in tqdm(dataset[\"resume\"].values):\n",
    "#     segment.append(pretreatment(content))\n",
    "    segment_user.append(list(jieba.cut(content)))\n",
    "dataset[\"text_user\"] = segment_user\n",
    "# user_set.to_csv(\"user_set_segment.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_word2vec(x):\n",
    "    '''\n",
    "    param: x is a list contain all the words\n",
    "    return: the trained model\n",
    "    '''\n",
    "\n",
    "    model = word2vec.Word2Vec(x, size=200, window=5, min_count=2, workers=8,\n",
    "                             iter=10, sg=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# w2v_model_1 = train_word2vec(dataset.text_job.values)\n",
    "# w2v_model_1.save('./word2vec1.model')\n",
    "w2v_model_1 = Word2Vec.load('./word2vec1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# w2v_model_2 = train_word2vec(dataset.text_user.values)\n",
    "# w2v_model_2.save('./word2vec2.model')\n",
    "w2v_model_2 = Word2Vec.load('./word2vec2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        '''\n",
    "        param: sentences: the list of corpus\n",
    "               sen_len: the max length of each sentence\n",
    "               w2v_path: the path storing word emnbedding model\n",
    "        '''\n",
    "\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word2vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding(\"\")\n",
    "        self.add_embedding(\"\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[''])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self):\n",
    "        '''\n",
    "        change words in sentences into idx in embedding_matrix\n",
    "        '''\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[''])\n",
    "            sentence_idx = self.pad_sentence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def labels_to_tensor(self, y):\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TextCNN\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, pool_size, dim, method='max'):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, kernel_size[0]),\n",
    "            nn.BatchNorm2d(channels), # 其中的参数是通道数\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(pool_size)\n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size[1]),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((1, dim)) # （1，dim）是指输出大小\n",
    "        )\n",
    "        if method == 'max':\n",
    "            self.pool = nn.AdaptiveMaxPool2d((1, dim))\n",
    "        elif method == 'mean':\n",
    "            self.pool = nn.AdaptiveAvgPool2d((1, dim))\n",
    "        else:\n",
    "            raise ValueError('method {} not exist'.format(method))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.net2(x).squeeze(2)\n",
    "        x = self.pool(x).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# PJFNN\n",
    "class PJFNN(nn.Module):\n",
    "    def __init__(self, embedding1,embedding2, channels=1, dropout=0.5, fix_embedding=True):\n",
    "        super(PJFNN, self).__init__()\n",
    "        self.dim1 = embedding1.size(1)\n",
    "#         self.user_dim = input_dim\n",
    "        self.dim2 = embedding2.size(1)\n",
    "        self.channels = channels\n",
    "        # job  字典中有(行数)个词，词向量维度为(列数)\n",
    "        self.embedding1 = nn.Embedding(embedding1.size(0), embedding1.size(1))\n",
    "        self.embedding1.weight = nn.Parameter(embedding1)\n",
    "        self.embedding1.weight.requires_grad = False if fix_embedding else True\n",
    "        # user\n",
    "        self.embedding2 = nn.Embedding(embedding2.size(0), embedding2.size(1))\n",
    "        self.embedding2.weight = nn.Parameter(embedding2)\n",
    "        self.embedding2.weight.requires_grad = False if fix_embedding else True\n",
    "\n",
    "        self.linear_transform = nn.Linear(200, 64)\n",
    "\n",
    "        self.geek_layer = TextCNN(\n",
    "            channels=self.channels,\n",
    "            kernel_size=[(5, 1), (5, 1)],\n",
    "            pool_size=(2, 1),\n",
    "            dim=200,\n",
    "            method='max'\n",
    "        )\n",
    "\n",
    "        self.job_layer = TextCNN(\n",
    "            channels=self.channels,\n",
    "            kernel_size=[(5, 1), (5, 1)],\n",
    "            pool_size=(2, 1),\n",
    "            dim=200,\n",
    "            method='mean'\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            input_size=128,\n",
    "            output_size=1,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, job, user):\n",
    "        job = self.embedding1(job.long()) #.long()\n",
    "        job = job.unsqueeze(1)\n",
    "        job = self.job_layer(job)\n",
    "\n",
    "        user = self.embedding2(user.long()) #.long()\n",
    "        user = user.unsqueeze(1)\n",
    "        user = self.geek_layer(user)\n",
    "\n",
    "        # MLP层\n",
    "#         user = self.user_layer(user)\n",
    "        # 全连接层,变成64维\n",
    "        user = self.linear_transform(user)\n",
    "        job = self.linear_transform(job)\n",
    "        # tensor进行拼接\n",
    "        x = torch.cat((user,job),dim=1)\n",
    "        # mlp层\n",
    "        x = self.mlp(x).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "制作dataset\n",
    "'''\n",
    "# 建立了dataset所需要的'__init__', '__getitem__', '__len__'\n",
    "# 好让dataloader能使用\n",
    "class JobUserDataset(data.Dataset):\n",
    "    def __init__(self, job, user, label):\n",
    "        self.job = job\n",
    "        self.user = user\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.job[idx], self.user[idx]\n",
    "        return self.job[idx], self.user[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [岗位职责, ：, \\n, ·,  , 业务学习, &, 任务, 处理, ：,  , 严格,...\n",
       "1         [在, 海轮, 上, 工作, 的, 人员, 统称, 海员, 。,  , 海员, 分, 两大类...\n",
       "2         [岗位职责, ：, \\n, 1, .,  , 响应, 用户, IT, 相关, 问题, 请求,...\n",
       "3         [帮助, 客户, 公司, 进行, 信息化, 咨询, 。, 具体, 是, 通过, SAP,  ...\n",
       "4                                              [负责, 起草, 文书]\n",
       "                                ...                        \n",
       "138233    [岗位职责, ：, \\n, 1, 、, 设备, 安装, 、, 调试, 和, 量产, ；, \\...\n",
       "138234    [作为, 零售, 经理, 储备, 梯队, ，, 负责, 店铺, 运营, 、, 人员, 、, ...\n",
       "138235    [岗位职责, ：, \\n, \\n, 1, 、, 根据, 基金, 经理, 的, 相关, 要求,...\n",
       "138236    [（, 1, ）, 从事, 水利水电, 、, 抽水, 蓄能, 、, 地质灾害, 、, 市政,...\n",
       "138237    [岗位职责, ：, \\n, 1, 、, 负责, 室内, 舒适性, 研究, \\n, 2, 、,...\n",
       "Name: text_job, Length: 138238, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text_job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in dataset['text_job']:\n",
    "#     print(i)\n",
    "    temp = str(i[1:-1]).split(',')\n",
    "    # 删除当前字符串的首尾的空格和换行符\n",
    "    text.append([t.strip()[1:-1] for t in temp])\n",
    "dataset['text_job'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in dataset['text_user']:\n",
    "    temp = str(i[1:-1]).split(',')\n",
    "    # 删除当前字符串的首尾的空格和换行符\n",
    "    text.append([t.strip()[1:-1] for t in temp])\n",
    "dataset['text_user'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_x_t = train_dataset['text_job']\n",
    "# train_user_t = train_dataset['text_user']\n",
    "# train_y_t = train_dataset['label']\n",
    "#\n",
    "# val_x_t = test_dataset['text_job']\n",
    "# val_user_t = test_dataset['text_user']\n",
    "# val_y_t = test_dataset['label']\n",
    "#\n",
    "# test_x_t = val_dataset['text_job']\n",
    "# test_user_t = val_dataset['text_user']\n",
    "# test_y_t = val_dataset['label']\n",
    "\n",
    "x_t = dataset['text_job']\n",
    "user_t = dataset['text_user']\n",
    "y_t = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2507/2263902633.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  self.embedding_matrix.append(self.embedding[word])\n",
      "/tmp/ipykernel_2507/2263902633.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  self.embedding_matrix = torch.tensor(self.embedding_matrix)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 28173\n",
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2507/2263902633.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  self.embedding_matrix.append(self.embedding[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 130209\n"
     ]
    }
   ],
   "source": [
    "sen_len_user = 50\n",
    "preprocess_user = Preprocess(x_t, sen_len_user, w2v_path=\"./word2vec1.model\")\n",
    "embedding1 = preprocess_user.make_embedding(load=True)\n",
    "x = preprocess_user.sentence_word2idx()\n",
    "\n",
    "sen_len = 200\n",
    "preprocess = Preprocess(user_t, sen_len, w2v_path=\"./word2vec2.model\")\n",
    "embedding2 = preprocess.make_embedding(load=True)\n",
    "user = preprocess.sentence_word2idx()\n",
    "\n",
    "y = preprocess_user.labels_to_tensor(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_val_split(x1,x2,y, ratio_train, ratio_test, ratio_val):\n",
    "    x1_train, x1_middle,x2_train, x2_middle,y_train, y_middle = train_test_split(x1,x2,y, test_size=1-ratio_train, random_state=20)\n",
    "    ratio = ratio_val/(ratio_test + ratio_val)\n",
    "    x1_test, x1_validation,x2_test, x2_validation,y_test, y_validation = train_test_split(x1_middle,x2_middle,y_middle, test_size=ratio, random_state=20)\n",
    "    return x1_train, x1_test, x1_validation,x2_train, x2_test, x2_validation,y_train, y_test, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset, val_dataset = train_test_val_split(dataset, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_x, test_x,val_x,train_user,test_user,val_user,train_y,test_y,val_y=train_test_val_split(x,user,y, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sen_len_user = 50\n",
    "# preprocess_user = Preprocess(val_x_t, sen_len_user, w2v_path=\"./word2vec1.model\")\n",
    "# embedding_user = preprocess_user.make_embedding(load=True)\n",
    "# val_x = preprocess_user.sentence_word2idx()\n",
    "#\n",
    "# sen_len = 200\n",
    "# preprocess = Preprocess(val_user_t, sen_len, w2v_path=\"./word2vec2.model\")\n",
    "# embedding2 = preprocess.make_embedding(load=True)\n",
    "# val_user = preprocess.sentence_word2idx()\n",
    "#\n",
    "# val_y = preprocess_user.labels_to_tensor(val_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sen_len_user = 50\n",
    "# preprocess_user = Preprocess(test_x_t, sen_len_user, w2v_path=\"./word2vec1.model\")\n",
    "# embedding1 = preprocess_user.make_embedding(load=True)\n",
    "# test_x = preprocess_user.sentence_word2idx()\n",
    "#\n",
    "# sen_len = 200\n",
    "# preprocess = Preprocess(test_user_t, sen_len, w2v_path=\"./word2vec2.model\")\n",
    "# embedding2 = preprocess.make_embedding(load=True)\n",
    "# test_user = preprocess.sentence_word2idx()\n",
    "#\n",
    "# test_y = preprocess_user.labels_to_tensor(test_y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset构建\n",
    "train_dataset = JobUserDataset(train_x, train_user, train_y)\n",
    "val_dataset = JobUserDataset(val_x, val_user, val_y)\n",
    "test_dataset = JobUserDataset(test_x, test_user, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # 一次训练所选取的样本数\n",
    "# dataset导入\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle = False)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader =DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def training(n_epoch, lr, train, valid, model, device, model_name, model_dir=\"./\"):\n",
    "    # summary model parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\nstart training, total parameter:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) #, weight_decay=1e-4\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch, eta_min=0, last_epoch=-1)\n",
    "    # total_loss, total_acc = 0, 0\n",
    "    best_acc, best_precision, best_recall, best_f1, best_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        pred_label = []\n",
    "        y_label = []\n",
    "        # training\n",
    "        for i, (jobs, users, labels) in enumerate(train):\n",
    "\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            # entities = entities.to(torch.float32)\n",
    "            # entities = entities.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO 是否考虑模型用多个优化器？\n",
    "            optimizer.zero_grad() # 将所有模型参数的梯度置为0\n",
    "            # model.zero_grad() # 除所有可训练的torch.Tensor的梯度\n",
    "            outputs = model(jobs, users)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_label.extend([0 if i<0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "        train_losses = total_loss/t_batch\n",
    "        train_acc = accuracy_score(y_label, pred_label)\n",
    "        train_precision = precision_score(y_label, pred_label)\n",
    "        train_recall = recall_score(y_label, pred_label)\n",
    "        train_auc = roc_auc_score(y_label, pred_label)\n",
    "        train_f1 = f1_score(y_label, pred_label)\n",
    "        print('[ Epoch{}: {}/{}] '.format(epoch+1, i+1, t_batch))\n",
    "        print('\\nTrain | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(train_losses,train_acc,train_precision, train_recall,train_auc,train_f1, time.time()-start_time))\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # pred_score = []\n",
    "            pred_label = []\n",
    "            y_label = []\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (jobs, users, labels) in enumerate(valid):\n",
    "                # 放GPU上运行\n",
    "                jobs = jobs.to(torch.float32)\n",
    "                jobs = jobs.to(device)\n",
    "\n",
    "                users = users.to(torch.float32)\n",
    "                users = users.to(device)\n",
    "\n",
    "                # entities = entities.to(torch.float32)\n",
    "                # entities = entities.to(device)\n",
    "\n",
    "                labels = labels.to(torch.float32)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(jobs, users)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                '''\n",
    "                存一下预测score\n",
    "                '''\n",
    "                # pred_score.extend([j for j in list(outputs.cpu().detach().numpy())])\n",
    "                pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "                y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "            # print('\\nVal | Loss:{:.5f} Time:{:.6f}'.format(total_loss/v_batch, time.time()-start_time))\n",
    "            val_losses = total_loss/v_batch\n",
    "            val_acc = accuracy_score(y_label, pred_label)\n",
    "            val_precision = precision_score(y_label, pred_label)\n",
    "            val_recall = recall_score(y_label, pred_label)\n",
    "            val_auc = roc_auc_score(y_label, pred_label)\n",
    "            val_f1 = f1_score(y_label, pred_label)\n",
    "            print('\\nVal | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(val_losses,val_acc,val_precision, val_recall,val_auc,val_f1, time.time()-start_time))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_precision = val_precision\n",
    "                best_recall = val_recall\n",
    "                best_f1 = val_f1\n",
    "                best_auc = val_auc\n",
    "                torch.save(model, \"{}/{}.model\".format(model_dir, model_name))\n",
    "                print('save model with acc: {:.3f}, recall: {:.3f}, auc: {:.3f}'.format(best_acc,best_recall,best_auc))\n",
    "        print('------------------------------------------------------')\n",
    "        # lr_scheduler.step()\n",
    "        # 将model的模式设为train，这样optimizer就可以更新model的參數（因為刚刚转为eval模式）\n",
    "        model.train()\n",
    "    return best_acc, best_precision, best_recall, best_f1, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fix_embedding = False\n",
    "# input_dim = train_dataset[0][1].shape[0]\n",
    "model = PJFNN(embedding1, embedding2, dropout=0.7, channels=32, fix_embedding=fix_embedding)\n",
    "epoch = 20\n",
    "lr = 0.0001\n",
    "model_dir = './'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'PJFNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, total parameter:31716849, trainable:31716849\n",
      "\n",
      "[ Epoch1: 2592/2592] \n",
      "\n",
      "Train | Loss:0.66579 ACC:0.57631 Precision:0.58125 Recall:0.55026 AUC:0.57634 F1:0.56533 Time:43.162059\n",
      "\n",
      "Val | Loss:0.60337 ACC:0.66026 Precision:0.68374 Recall:0.58670 AUC:0.65971 F1:0.63152 Time:46.856344\n",
      "save model with acc: 0.660, recall: 0.587, auc: 0.660\n",
      "------------------------------------------------------\n",
      "[ Epoch2: 2592/2592] \n",
      "\n",
      "Train | Loss:0.59969 ACC:0.66753 Precision:0.68806 Recall:0.61470 AUC:0.66760 F1:0.64931 Time:43.385087\n",
      "\n",
      "Val | Loss:0.56625 ACC:0.69466 Precision:0.72480 Recall:0.62009 AUC:0.69410 F1:0.66837 Time:47.012575\n",
      "save model with acc: 0.695, recall: 0.620, auc: 0.694\n",
      "------------------------------------------------------\n",
      "[ Epoch3: 2592/2592] \n",
      "\n",
      "Train | Loss:0.56488 ACC:0.70387 Precision:0.72231 Recall:0.66377 AUC:0.70392 F1:0.69180 Time:43.724545\n",
      "\n",
      "Val | Loss:0.54505 ACC:0.71908 Precision:0.72749 Recall:0.69371 AUC:0.71888 F1:0.71020 Time:47.402593\n",
      "save model with acc: 0.719, recall: 0.694, auc: 0.719\n",
      "------------------------------------------------------\n",
      "[ Epoch4: 2592/2592] \n",
      "\n",
      "Train | Loss:0.54027 ACC:0.72489 Precision:0.74123 Recall:0.69225 AUC:0.72494 F1:0.71590 Time:44.034817\n",
      "\n",
      "Val | Loss:0.53220 ACC:0.73032 Precision:0.75408 Recall:0.67745 AUC:0.72993 F1:0.71372 Time:47.790246\n",
      "save model with acc: 0.730, recall: 0.677, auc: 0.730\n",
      "------------------------------------------------------\n",
      "[ Epoch5: 2592/2592] \n",
      "\n",
      "Train | Loss:0.52134 ACC:0.73878 Precision:0.75281 Recall:0.71217 AUC:0.73882 F1:0.73192 Time:43.579872\n",
      "\n",
      "Val | Loss:0.52326 ACC:0.73626 Precision:0.74174 Recall:0.71871 AUC:0.73612 F1:0.73005 Time:47.292233\n",
      "save model with acc: 0.736, recall: 0.719, auc: 0.736\n",
      "------------------------------------------------------\n",
      "[ Epoch6: 2592/2592] \n",
      "\n",
      "Train | Loss:0.50513 ACC:0.75162 Precision:0.76009 Recall:0.73639 AUC:0.75164 F1:0.74805 Time:43.513662\n",
      "\n",
      "Val | Loss:0.51918 ACC:0.74023 Precision:0.73832 Recall:0.73810 AUC:0.74022 F1:0.73821 Time:47.007820\n",
      "save model with acc: 0.740, recall: 0.738, auc: 0.740\n",
      "------------------------------------------------------\n",
      "[ Epoch7: 2592/2592] \n",
      "\n",
      "Train | Loss:0.49031 ACC:0.75893 Precision:0.76289 Recall:0.75240 AUC:0.75894 F1:0.75761 Time:43.484854\n",
      "\n",
      "Val | Loss:0.51973 ACC:0.74114 Precision:0.74706 Recall:0.72316 AUC:0.74100 F1:0.73492 Time:47.199048\n",
      "save model with acc: 0.741, recall: 0.723, auc: 0.741\n",
      "------------------------------------------------------\n",
      "[ Epoch8: 2592/2592] \n",
      "\n",
      "Train | Loss:0.47669 ACC:0.76784 Precision:0.76639 Recall:0.77152 AUC:0.76783 F1:0.76895 Time:43.485650\n",
      "\n",
      "Val | Loss:0.51903 ACC:0.74154 Precision:0.74292 Recall:0.73263 AUC:0.74147 F1:0.73774 Time:47.201560\n",
      "save model with acc: 0.742, recall: 0.733, auc: 0.741\n",
      "------------------------------------------------------\n",
      "[ Epoch9: 2592/2592] \n",
      "\n",
      "Train | Loss:0.46437 ACC:0.77539 Precision:0.76910 Recall:0.78799 AUC:0.77537 F1:0.77843 Time:43.420136\n",
      "\n",
      "Val | Loss:0.52386 ACC:0.74396 Precision:0.74283 Recall:0.74029 AUC:0.74393 F1:0.74156 Time:47.178166\n",
      "save model with acc: 0.744, recall: 0.740, auc: 0.744\n",
      "------------------------------------------------------\n",
      "[ Epoch10: 2592/2592] \n",
      "\n",
      "Train | Loss:0.45029 ACC:0.78415 Precision:0.77197 Recall:0.80742 AUC:0.78412 F1:0.78930 Time:43.688858\n",
      "\n",
      "Val | Loss:0.53105 ACC:0.74367 Precision:0.73588 Recall:0.75406 AUC:0.74375 F1:0.74486 Time:47.265957\n",
      "------------------------------------------------------\n",
      "[ Epoch11: 2592/2592] \n",
      "\n",
      "Train | Loss:0.43833 ACC:0.79130 Precision:0.77394 Recall:0.82384 AUC:0.79125 F1:0.79811 Time:43.671250\n",
      "\n",
      "Val | Loss:0.54333 ACC:0.74172 Precision:0.73823 Recall:0.74291 AUC:0.74173 F1:0.74056 Time:47.157988\n",
      "------------------------------------------------------\n",
      "[ Epoch12: 2592/2592] \n",
      "\n",
      "Train | Loss:0.42548 ACC:0.79961 Precision:0.77697 Recall:0.84128 AUC:0.79955 F1:0.80785 Time:43.409335\n",
      "\n",
      "Val | Loss:0.54639 ACC:0.74121 Precision:0.73257 Recall:0.75355 AUC:0.74130 F1:0.74291 Time:47.128855\n",
      "------------------------------------------------------\n",
      "[ Epoch13: 2592/2592] \n",
      "\n",
      "Train | Loss:0.41465 ACC:0.80559 Precision:0.77951 Recall:0.85303 AUC:0.80552 F1:0.81461 Time:43.246901\n",
      "\n",
      "Val | Loss:0.56810 ACC:0.73817 Precision:0.72775 Recall:0.75465 AUC:0.73830 F1:0.74096 Time:46.981405\n",
      "------------------------------------------------------\n",
      "[ Epoch14: 2592/2592] \n",
      "\n",
      "Train | Loss:0.40303 ACC:0.81336 Precision:0.78417 Recall:0.86547 AUC:0.81329 F1:0.82282 Time:43.302178\n",
      "\n",
      "Val | Loss:0.58170 ACC:0.73694 Precision:0.72268 Recall:0.76245 AUC:0.73714 F1:0.74203 Time:47.031101\n",
      "------------------------------------------------------\n",
      "[ Epoch15: 2592/2592] \n",
      "\n",
      "Train | Loss:0.39237 ACC:0.81995 Precision:0.78748 Recall:0.87713 AUC:0.81986 F1:0.82989 Time:43.436544\n",
      "\n",
      "Val | Loss:0.60891 ACC:0.73485 Precision:0.73243 Recall:0.73365 AUC:0.73484 F1:0.73304 Time:47.128469\n",
      "------------------------------------------------------\n",
      "[ Epoch16: 2592/2592] \n",
      "\n",
      "Train | Loss:0.38160 ACC:0.82737 Precision:0.79385 Recall:0.88510 AUC:0.82729 F1:0.83699 Time:43.375285\n",
      "\n",
      "Val | Loss:0.62458 ACC:0.73394 Precision:0.72407 Recall:0.74940 AUC:0.73406 F1:0.73651 Time:46.957825\n",
      "------------------------------------------------------\n",
      "[ Epoch17: 2592/2592] \n",
      "\n",
      "Train | Loss:0.37006 ACC:0.83317 Precision:0.79720 Recall:0.89434 AUC:0.83308 F1:0.84298 Time:43.426967\n",
      "\n",
      "Val | Loss:0.63847 ACC:0.73177 Precision:0.72136 Recall:0.74860 AUC:0.73190 F1:0.73473 Time:46.998644\n",
      "------------------------------------------------------\n",
      "[ Epoch18: 2592/2592] \n",
      "\n",
      "Train | Loss:0.35967 ACC:0.83923 Precision:0.80205 Recall:0.90137 AUC:0.83913 F1:0.84882 Time:43.351846\n",
      "\n",
      "Val | Loss:0.66007 ACC:0.73362 Precision:0.72127 Recall:0.75487 AUC:0.73378 F1:0.73769 Time:46.925809\n",
      "------------------------------------------------------\n",
      "[ Epoch19: 2592/2592] \n",
      "\n",
      "Train | Loss:0.35086 ACC:0.84328 Precision:0.80540 Recall:0.90588 AUC:0.84319 F1:0.85269 Time:43.104557\n",
      "\n",
      "Val | Loss:0.65486 ACC:0.72707 Precision:0.70023 Recall:0.78679 AUC:0.72752 F1:0.74099 Time:46.807127\n",
      "------------------------------------------------------\n",
      "[ Epoch20: 2592/2592] \n",
      "\n",
      "Train | Loss:0.34208 ACC:0.84876 Precision:0.80925 Recall:0.91322 AUC:0.84867 F1:0.85810 Time:44.707280\n",
      "\n",
      "Val | Loss:0.67575 ACC:0.72848 Precision:0.70676 Recall:0.77389 AUC:0.72882 F1:0.73881 Time:48.497305\n",
      "------------------------------------------------------\n",
      "best_acc 0.7439597800925926\n",
      "best_precision 0.7428320655354008\n",
      "best_recall 0.7428320655354008\n",
      "best_f1 0.7415574458763827\n",
      "best_auc 0.7439320952874672\n"
     ]
    }
   ],
   "source": [
    "best_acc, best_precision, best_recall, best_f1, best_auc = training(epoch, lr, train_loader, val_loader, model, device, model_name, model_dir)\n",
    "\n",
    "# 输出结果（验证集）\n",
    "print('best_acc',best_acc)\n",
    "print('best_precision',best_precision)\n",
    "print('best_recall',best_precision)\n",
    "print('best_f1',best_f1)\n",
    "print('best_auc',best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "    pred_label = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (jobs, users, labels) in enumerate(test_loader):\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            # entities = entities.to(torch.float32)\n",
    "            # entities = entities.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(jobs, users)\n",
    "\n",
    "            pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "\n",
    "        test_acc = accuracy_score(y_label, pred_label)\n",
    "        test_precision = precision_score(y_label, pred_label)\n",
    "        test_recall = recall_score(y_label, pred_label)\n",
    "        test_auc = roc_auc_score(y_label, pred_label)\n",
    "        test_f1 = f1_score(y_label, pred_label)\n",
    "    return test_acc, test_auc, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc 0.7421513310185185\n",
      "test_precision 0.7405768132495717\n",
      "test_recall 0.7405768132495717\n",
      "test_f1 0.7442694694551064\n",
      "test_auc 0.7421322330311165\n"
     ]
    }
   ],
   "source": [
    "# 输出结果(测试集)\n",
    "test_acc, test_auc, test_precision, test_recall, test_f1 = testing(\n",
    "    torch.load('/root/PJFNN.model'), test_loader)\n",
    "print('test_acc', test_acc)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_precision)\n",
    "print('test_f1', test_f1)\n",
    "print('test_auc', test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
