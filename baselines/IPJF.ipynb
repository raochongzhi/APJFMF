{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset_user_job_all_1.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee9eea33210ec11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EMBED_DIM = 200"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b23f176ef931218"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_words = [line.strip() for line in open('chinese_stopword.txt',encoding='UTF-8').readlines()]\n",
    "\n",
    "def pretreatment(comment):\n",
    "\n",
    "    token_words = jieba.lcut(comment)\n",
    "    token_words = [w for w in token_words if w not in stop_words]\n",
    "    token_words =  pseg.cut(' '.join(token_words))\n",
    "    cleaned_word = []\n",
    "    for word, tag in token_words:\n",
    "        if word.isdigit():\n",
    "            continue\n",
    "        else:\n",
    "            cleaned_word.append(word)\n",
    "    return cleaned_word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f3bf0df404e0c72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segment_job =[]\n",
    "# job_set=pd.read_csv('job_information.csv')\n",
    "for content in tqdm(dataset[\"岗位描述\"].values):\n",
    "#     segment.append(pretreatment(content))\n",
    "    segment_job.append(list(jieba.cut(content)))\n",
    "dataset[\"text_job\"] = segment_job\n",
    "# job_set.to_csv(\"job_set_segment.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acf4e0ca67502152"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "segment_user = []\n",
    "# job_set=pd.read_csv('job_information.csv')\n",
    "for content in tqdm(dataset[\"resume\"].values):\n",
    "#     segment.append(pretreatment(content))\n",
    "    segment_user.append(list(jieba.cut(content)))\n",
    "dataset[\"text_user\"] = segment_user\n",
    "# user_set.to_csv(\"user_set_segment.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4e549cf934e4b33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_word2vec(x):\n",
    "    '''\n",
    "    param: x is a list contain all the words\n",
    "    return: the trained model\n",
    "    '''\n",
    "\n",
    "    model = word2vec.Word2Vec(x, size=200, window=5, min_count=2, workers=8, iter=10, sg=1)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f8e1284c8d906a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# w2v_model_1 = train_word2vec(dataset.text_job.values)\n",
    "# w2v_model_1.save('./word2vec1.model')\n",
    "w2v_model_1 = Word2Vec.load('./word2vec1.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b58215657243db6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# w2v_model_2 = train_word2vec(dataset.text_user.values)\n",
    "# w2v_model_2.save('./word2vec2.model')\n",
    "w2v_model_2 = Word2Vec.load('./word2vec2.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3a1dc69f0b3b2d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        '''\n",
    "        param: sentences: the list of corpus\n",
    "               sen_len: the max length of each sentence\n",
    "               w2v_path: the path storing word emnbedding model\n",
    "        '''\n",
    "\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word2vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding(\"\")\n",
    "        self.add_embedding(\"\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[''])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self):\n",
    "        '''\n",
    "        change words in sentences into idx in embedding_matrix\n",
    "        '''\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[''])\n",
    "            sentence_idx = self.pad_sentence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def labels_to_tensor(self, y):\n",
    "        return torch.LongTensor(y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17e4ce3cb8c96c9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class IPJF(torch.nn.Module):\n",
    "    def __init__(self, word_embeddings1, word_embeddings2):\n",
    "        super(IPJF, self).__init__()\n",
    "\n",
    "        # embedding_matrix = [[0...0], [...], ...[]]\n",
    "        self.Word_Embeds1 = torch.nn.Embedding.from_pretrained(word_embeddings1, padding_idx=0)\n",
    "        self.Word_Embeds1.weight.requires_grad = False\n",
    "        \n",
    "        self.Word_Embeds2 = torch.nn.Embedding.from_pretrained(word_embeddings2, padding_idx=0)\n",
    "        self.Word_Embeds2.weight.requires_grad = False\n",
    "        \n",
    "        self.Expect_ConvNet = torch.nn.Sequential(\n",
    "            # in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,...\n",
    "            torch.nn.Conv1d(in_channels=EMBED_DIM, out_channels=EMBED_DIM, kernel_size=5),\n",
    "            # BatchNorm1d只处理第二个维度\n",
    "            # torch.nn.BatchNorm1d(EMBED_DIM),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool1d(kernel_size=3),\n",
    "\n",
    "            # in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,...\n",
    "            # torch.nn.Conv1d(in_channels=EMBED_DIM, out_channels=EMBED_DIM, kernel_size=5),\n",
    "            # # BatchNorm1d只处理第二个维度\n",
    "            # torch.nn.BatchNorm1d(EMBED_DIM),\n",
    "            # torch.nn.ReLU(inplace=True),\n",
    "            # torch.nn.MaxPool1d(kernel_size=50)\n",
    "        )\n",
    "\n",
    "        self.Job_ConvNet = torch.nn.Sequential(\n",
    "            # in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,...\n",
    "            torch.nn.Conv1d(in_channels=EMBED_DIM, out_channels=EMBED_DIM, kernel_size=5),\n",
    "            # BatchNorm1d只处理第二个维度\n",
    "            # torch.nn.BatchNorm1d(EMBED_DIM),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "\n",
    "            # in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1,...\n",
    "            # torch.nn.Conv1d(in_channels=EMBED_DIM, out_channels=EMBED_DIM, kernel_size=3),\n",
    "            # # BatchNorm1d只处理第二个维度\n",
    "            # torch.nn.BatchNorm1d(EMBED_DIM),\n",
    "            # torch.nn.ReLU(inplace=True),\n",
    "            # torch.nn.MaxPool1d(kernel_size=50)\n",
    "        )\n",
    "\n",
    "        # match mlp\n",
    "        self.Match_MLP = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * EMBED_DIM, EMBED_DIM),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(EMBED_DIM, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # [batch_size *2, MAX_PROFILELEN, MAX_TERMLEN] = (40, 15, 50)\n",
    "    # term: padding same, word: padding 0\n",
    "    # expects_sample, jobs_sample are in same format\n",
    "    def forward(self, expects, jobs):\n",
    "\n",
    "        # word level:\n",
    "        # [batch_size, MAX_PROFILELEN, MAX_TERMLEN] (40, 15, 50) ->\n",
    "        # [batch_size, MAX_PROFILELEN * MAX_TERMLEN](40 * 15, 50)\n",
    "        shape = expects.shape\n",
    "        expects_, jobs_ = expects.view([shape[0], -1]), jobs.view([shape[0], -1])\n",
    "\n",
    "        # embeddings: [batch_size, MAX_PROFILELEN * MAX_TERMLEN, EMBED_DIM]\n",
    "        \n",
    "        jobs_wordembed = self.Word_Embeds1(jobs_).float()\n",
    "        \n",
    "        expects_wordembed = self.Word_Embeds2(expects_).float()\n",
    "        \n",
    "        # permute for conv1d\n",
    "        # embeddings: [batch_size, EMBED_DIM, MAX_PROFILELEN * MAX_TERMLEN]\n",
    "        expects_wordembed_ = expects_wordembed.permute(0, 2, 1)\n",
    "        jobs_wordembed_ = jobs_wordembed.permute(0, 2, 1)\n",
    "\n",
    "        # [batch_size, EMBED_DIM, x]\n",
    "        expect_convs_out = self.Expect_ConvNet(expects_wordembed_)\n",
    "        job_convs_out = self.Job_ConvNet(jobs_wordembed_)\n",
    "\n",
    "        # [batch_size, EMBED_DIM, x] -> [batch_size, EMBED_DIM, 1]\n",
    "        expect_len, job_len = expect_convs_out.shape[-1], job_convs_out.shape[-1]\n",
    "        expect_final_out = torch.nn.AvgPool1d(kernel_size=expect_len)(expect_convs_out).squeeze(-1)\n",
    "        job_final_out = torch.nn.MaxPool1d(kernel_size=job_len)(job_convs_out).squeeze(-1)\n",
    "\n",
    "        return self.Match_MLP(torch.cat([expect_final_out, job_final_out], dim=-1)).squeeze(-1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5564c4c221b055ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "制作dataset\n",
    "'''\n",
    "# 建立了dataset所需要的'__init__', '__getitem__', '__len__'\n",
    "# 好让dataloader能使用\n",
    "class JobUserDataset(data.Dataset):\n",
    "    def __init__(self, job, user, label):\n",
    "        self.job = job\n",
    "        self.user = user\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.job[idx], self.user[idx]\n",
    "        return self.job[idx], self.user[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.job)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98682216f1a0a856"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in dataset['text_job']:\n",
    "#     print(i)\n",
    "    temp = str(i[1:-1]).split(',')\n",
    "    # 删除当前字符串的首尾的空格和换行符\n",
    "    text.append([t.strip()[1:-1] for t in temp])\n",
    "dataset['text_job'] = text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cd1e0ba935b5421"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in dataset['text_user']:\n",
    "    temp = str(i[1:-1]).split(',')\n",
    "    # 删除当前字符串的首尾的空格和换行符\n",
    "    text.append([t.strip()[1:-1] for t in temp])\n",
    "dataset['text_user'] = text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "251c2ecd7e09f217"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_t = dataset['text_job']\n",
    "user_t = dataset['text_user']\n",
    "y_t = dataset['label']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67942ebd2b4c9fd1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sen_len_job = 50\n",
    "preprocess_user = Preprocess(x_t, sen_len_job, w2v_path=\"./word2vec1.model\")\n",
    "embedding1 = preprocess_user.make_embedding(load=True)\n",
    "x = preprocess_user.sentence_word2idx()\n",
    "\n",
    "sen_len_user = 200\n",
    "preprocess = Preprocess(user_t, sen_len_user, w2v_path=\"./word2vec2.model\")\n",
    "embedding2 = preprocess.make_embedding(load=True)\n",
    "user = preprocess.sentence_word2idx()\n",
    "\n",
    "y = preprocess_user.labels_to_tensor(y_t)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc0313d249eb6a86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_test_val_split(x1,x2,y, ratio_train, ratio_test, ratio_val):\n",
    "    x1_train, x1_middle,x2_train, x2_middle,y_train, y_middle = train_test_split(x1,x2,y, test_size=1-ratio_train, random_state=20)\n",
    "    ratio = ratio_val/(ratio_test + ratio_val)\n",
    "    x1_test, x1_validation,x2_test, x2_validation,y_test, y_validation = train_test_split(x1_middle,x2_middle,y_middle, test_size=ratio, random_state=20)\n",
    "    return x1_train, x1_test, x1_validation,x2_train, x2_test, x2_validation,y_train, y_test, y_validation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7277388b075df327"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_x, test_x,val_x,train_user,test_user,val_user,train_y,test_y,val_y=train_test_val_split(x,user,y, 0.6, 0.2, 0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26126a7d5445b603"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset构建\n",
    "train_dataset = JobUserDataset(train_x, train_user, train_y)\n",
    "val_dataset = JobUserDataset(val_x, val_user, val_y)\n",
    "test_dataset = JobUserDataset(test_x, test_user, test_y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35b8798d7e1bf4bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32 # 一次训练所选取的样本数\n",
    "# dataset导入\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle = False)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader =DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e490a0bb83db075c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training(n_epoch, lr, train, valid, model, device, model_name, model_dir=\"./\"):\n",
    "    # summary model parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\nstart training, total parameter:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) #, weight_decay=1e-4\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch, eta_min=0, last_epoch=-1)\n",
    "    # total_loss, total_acc = 0, 0\n",
    "    best_acc, best_precision, best_recall, best_f1, best_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        pred_label = []\n",
    "        y_label = []\n",
    "        # training\n",
    "        for i, (jobs, users, labels) in enumerate(train):\n",
    "\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # TODO 是否考虑模型用多个优化器？\n",
    "            optimizer.zero_grad() # 将所有模型参数的梯度置为0\n",
    "            # model.zero_grad() # 除所有可训练的torch.Tensor的梯度\n",
    "            outputs = model(jobs, users)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_label.extend([0 if i<0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "        train_losses = total_loss/t_batch\n",
    "        train_acc = accuracy_score(y_label, pred_label)\n",
    "        train_precision = precision_score(y_label, pred_label)\n",
    "        train_recall = recall_score(y_label, pred_label)\n",
    "        train_auc = roc_auc_score(y_label, pred_label)\n",
    "        train_f1 = f1_score(y_label, pred_label)\n",
    "        print('[ Epoch{}: {}/{}] '.format(epoch+1, i+1, t_batch))\n",
    "        print('\\nTrain | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(train_losses,train_acc,train_precision, train_recall,train_auc,train_f1, time.time()-start_time))\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # pred_score = []\n",
    "            pred_label = []\n",
    "            y_label = []\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (jobs, users, labels) in enumerate(valid):\n",
    "                # 放GPU上运行\n",
    "                jobs = jobs.to(torch.float32)\n",
    "                jobs = jobs.to(device)\n",
    "\n",
    "                users = users.to(torch.float32)\n",
    "                users = users.to(device)\n",
    "\n",
    "                labels = labels.to(torch.float32)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(jobs, users)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                '''\n",
    "                存一下预测score\n",
    "                '''\n",
    "                # pred_score.extend([j for j in list(outputs.cpu().detach().numpy())])\n",
    "                pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "                y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "            # print('\\nVal | Loss:{:.5f} Time:{:.6f}'.format(total_loss/v_batch, time.time()-start_time))\n",
    "            val_losses = total_loss/v_batch\n",
    "            val_acc = accuracy_score(y_label, pred_label)\n",
    "            val_precision = precision_score(y_label, pred_label)\n",
    "            val_recall = recall_score(y_label, pred_label)\n",
    "            val_auc = roc_auc_score(y_label, pred_label)\n",
    "            val_f1 = f1_score(y_label, pred_label)\n",
    "            print('\\nVal | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(val_losses,val_acc,val_precision, val_recall,val_auc,val_f1, time.time()-start_time))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_precision = val_precision\n",
    "                best_recall = val_recall\n",
    "                best_f1 = val_f1\n",
    "                best_auc = val_auc\n",
    "                torch.save(model, \"{}/{}.model\".format(model_dir, model_name))\n",
    "                print('save model with acc: {:.3f}, recall: {:.3f}, auc: {:.3f}'.format(best_acc,best_recall,best_auc))\n",
    "        print('------------------------------------------------------')\n",
    "        # lr_scheduler.step()\n",
    "        # 将model的模式设为train，这样optimizer就可以更新model的參數（因為刚刚转为eval模式）\n",
    "        model.train()\n",
    "    return best_acc, best_precision, best_recall, best_f1, best_auc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df5b878273d38732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fix_embedding = False\n",
    "# input_dim = train_dataset[0][1].shape[0]\n",
    "model = IPJF(embedding1,embedding2)\n",
    "epoch = 20\n",
    "lr = 0.0001\n",
    "model_dir = './'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'IPJF'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6ca0442b3a9c575"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_acc, best_precision, best_recall, best_f1, best_auc = training(epoch, lr, train_loader, val_loader, model, device, model_name, model_dir)\n",
    "\n",
    "# 输出结果（验证集）\n",
    "print('best_acc',best_acc)\n",
    "print('best_precision',best_precision)\n",
    "print('best_recall',best_precision)\n",
    "print('best_f1',best_f1)\n",
    "print('best_auc',best_auc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10dae12db1311097"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "    pred_label = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (jobs, users, labels) in enumerate(test_loader):\n",
    "            # 放GPU上运行\n",
    "            jobs = jobs.to(torch.float32)\n",
    "            jobs = jobs.to(device)\n",
    "\n",
    "            users = users.to(torch.float32)\n",
    "            users = users.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(jobs, users)\n",
    "\n",
    "            pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "\n",
    "        test_acc = accuracy_score(y_label, pred_label)\n",
    "        test_precision = precision_score(y_label, pred_label)\n",
    "        test_recall = recall_score(y_label, pred_label)\n",
    "        test_auc = roc_auc_score(y_label, pred_label)\n",
    "        test_f1 = f1_score(y_label, pred_label)\n",
    "    return test_acc, test_auc, test_precision, test_recall, test_f1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c861b5802906444"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 输出结果(测试集)\n",
    "test_acc, test_auc, test_precision, test_recall, test_f1 = testing(\n",
    "    torch.load('IPJF.model'), test_loader)\n",
    "print('test_acc', test_acc)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_precision)\n",
    "print('test_f1', test_f1)\n",
    "print('test_auc', test_auc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1107232ae90da42d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
