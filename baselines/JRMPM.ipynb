{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T13:45:04.785810800Z",
     "start_time": "2023-08-22T13:45:02.925169800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import jieba.posseg as pseg\n",
    "import jieba\n",
    "\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T13:45:13.794665Z",
     "start_time": "2023-08-22T13:45:04.785810800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset_user_job_all_1.csv', dtype = {'UserID': 'str', 'JobID': 'str','label': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T13:46:26.242826700Z",
     "start_time": "2023-08-22T13:45:13.794665Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138238 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.808 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 138238/138238 [02:24<00:00, 956.71it/s] \n"
     ]
    }
   ],
   "source": [
    "segment_job =[]\n",
    "# job_set=pd.read_csv('job_information.csv')\n",
    "for content in tqdm(dataset[\"岗位描述\"].values):\n",
    "#     segment.append(pretreatment(content))\n",
    "    segment_job.append(list(jieba.cut(content)))\n",
    "dataset[\"text_job\"] = segment_job\n",
    "# job_set.to_csv(\"job_set_segment.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T13:52:36.560657700Z",
     "start_time": "2023-08-22T13:46:30.267294500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138238/138238 [12:47<00:00, 180.22it/s]\n"
     ]
    }
   ],
   "source": [
    "segment_user = []\n",
    "# job_set=pd.read_csv('job_information.csv')\n",
    "for content in tqdm(dataset[\"resume\"].values):\n",
    "#     segment.append(pretreatment(content))\n",
    "    segment_user.append(list(jieba.cut(content)))\n",
    "dataset[\"text_user\"] = segment_user\n",
    "# user_set.to_csv(\"user_set_segment.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:07:33.728035200Z",
     "start_time": "2023-08-22T14:07:33.363746900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 历史投递数据\n",
    "dataset_pos = dataset[dataset.label == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:07:37.987491Z",
     "start_time": "2023-08-22T14:07:37.355939800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9091/9091 [00:03<00:00, 2331.71it/s] \n"
     ]
    }
   ],
   "source": [
    "# 按照 userid 进行group\n",
    "memory_list_u = {}\n",
    "memory_num_u = {}\n",
    "groups = dataset_pos.groupby(\"UserID\")\n",
    "for idx, group in tqdm(groups):\n",
    "    # print(group.text_job)\n",
    "    seq_len = len(group)\n",
    "    memory_num_u[idx] = seq_len\n",
    "    seq_list = []\n",
    "    for text in group.text_job: #.unique().tolist()\n",
    "        seq_list.append(text)\n",
    "    memory_list_u[idx] = seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:07:55.704954900Z",
     "start_time": "2023-08-22T14:07:40.728476200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18685/18685 [00:03<00:00, 5566.15it/s] \n"
     ]
    }
   ],
   "source": [
    "# 按照 jobid 进行group\n",
    "memory_list_j = {}\n",
    "memory_num_j = {}\n",
    "groups = dataset_pos.groupby(\"JobID\")\n",
    "for idx, group in tqdm(groups):\n",
    "    seq_len = len(group)\n",
    "    memory_num_j[idx] = seq_len\n",
    "    seq_list = []\n",
    "    for text in group.text_user: #.unique().tolist()\n",
    "        seq_list.append(text)\n",
    "    memory_list_j[idx] = seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:08:13.793564400Z",
     "start_time": "2023-08-22T14:08:12.059975Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "list4 = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    userid = dataset.loc[i,'UserID']\n",
    "    jobid = dataset.loc[i, 'JobID']\n",
    "    list1.append(memory_num_u[userid])\n",
    "    list2.append(memory_list_u[userid])\n",
    "    try:\n",
    "        list3.append(memory_num_j[jobid])\n",
    "        list4.append(memory_list_j[jobid])\n",
    "    except:\n",
    "        list3.append(0)\n",
    "        list4.append([])\n",
    "\n",
    "dataset['memory_num_u'] = list1\n",
    "dataset['memory_list_u'] = list2\n",
    "dataset['memory_num_j'] = list3\n",
    "dataset['memory_list_j'] =list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:36:02.771223Z",
     "start_time": "2023-08-22T14:36:02.739508Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        '''\n",
    "        param: sentences: the list of corpus\n",
    "               sen_len: the max length of each sentence\n",
    "               w2v_path: the path storing word emnbedding model\n",
    "        '''\n",
    "\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word2vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding(\"\")\n",
    "        self.add_embedding(\"\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[''])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self):\n",
    "        '''\n",
    "        change words in sentences into idx in embedding_matrix\n",
    "        '''\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[''])\n",
    "            sentence_idx = self.pad_sentence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def labels_to_tensor(self, y):\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T15:06:24.029865300Z",
     "start_time": "2023-08-22T15:06:24.027864700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Preprocess1():\n",
    "    def __init__(self, sentences_list, sen_len, w2v_path=\"./w2v.model\"):\n",
    "        '''\n",
    "        param: sentences: the list of corpus\n",
    "               sen_len: the max length of each sentence\n",
    "               w2v_path: the path storing word emnbedding model\n",
    "        '''\n",
    "\n",
    "        self.w2v_path = w2v_path\n",
    "        self.sentences_list = sentences_list\n",
    "        self.sen_len = sen_len\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    def get_w2v_model(self):\n",
    "        self.embedding = Word2Vec.load(self.w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def make_embedding(self, load=True):\n",
    "        print(\"Get embedding ...\")\n",
    "        if load:\n",
    "            print(\"loading word2vec model ...\")\n",
    "            self.get_w2v_model()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding(\"\")\n",
    "        self.add_embedding(\"\")\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[''])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "\n",
    "    def sentence_word2idx(self, sentences):\n",
    "        '''\n",
    "        change words in sentences into idx in embedding_matrix\n",
    "        '''\n",
    "        sentence_list = []\n",
    "\n",
    "        # padding and cutting\n",
    "        if len(sentences) < 10:\n",
    "            for i in range(10-len(sentences)):\n",
    "                sentences.append('')\n",
    "        elif len(sentences) > 10:\n",
    "            sentences = sentences[: 10]\n",
    "\n",
    "        for i, sen in enumerate(sentences):\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if (word in self.word2idx.keys()):\n",
    "                    sentence_idx.append(self.word2idx[word])\n",
    "                else:\n",
    "                    sentence_idx.append(self.word2idx[''])\n",
    "            sentence_idx = self.pad_sentence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "\n",
    "    def sentencelist_2idx(self):\n",
    "        return torch.stack([self.sentence_word2idx(sentence) for sentence in self.sentences_list], dim=0)\n",
    "\n",
    "    def labels_to_tensor(self, y):\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:36:12.284466900Z",
     "start_time": "2023-08-22T14:36:12.271923100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JobUserDataset(data.Dataset):\n",
    "    def __init__(self, job, job_m, job_len, user, user_m, user_len, label):\n",
    "        self.job = job\n",
    "        self.job_m = job_m\n",
    "        self.job_len = job_len\n",
    "        self.user = user\n",
    "        self.user_m = user_m\n",
    "        self.user_len = user_len\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None:\n",
    "            return self.job[idx], self.job_m[idx], self.job_len[idx],self.user[idx], self.user_m[idx], self.user_len[idx]\n",
    "        return self.job[idx], self.job_m[idx], self.job_len[idx],self.user[idx], self.user_m[idx], self.user_len[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:36:15.405078300Z",
     "start_time": "2023-08-22T14:36:15.389433500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x_t1 = dataset['text_job']\n",
    "x_t2 = dataset['memory_num_u']\n",
    "x_t3 = dataset['memory_list_u']\n",
    "\n",
    "user_t1 = dataset['text_user']\n",
    "user_t2 = dataset['memory_num_j']\n",
    "user_t3 = dataset['memory_list_j']\n",
    "\n",
    "y_t = dataset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T14:36:38.108060300Z",
     "start_time": "2023-08-22T14:36:17.763156600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875/2263902633.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  self.embedding_matrix.append(self.embedding[word])\n",
      "/tmp/ipykernel_875/2263902633.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  self.embedding_matrix = torch.tensor(self.embedding_matrix)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 137703\n"
     ]
    }
   ],
   "source": [
    "sen_len_job = 200\n",
    "preprocess_job1 = Preprocess(x_t1, sen_len_job, w2v_path=\"word2vec_model/word2vec_shared.model\")\n",
    "embedding1 = preprocess_job1.make_embedding(load=True)\n",
    "job1 = preprocess_job1.sentence_word2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embedding1,\"baseline_dataset/embedding3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = torch.load(\"baseline_dataset/embedding3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T15:07:51.868855200Z",
     "start_time": "2023-08-22T15:06:49.032135900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875/1316519862.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  self.embedding_matrix.append(self.embedding[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 137703\n"
     ]
    }
   ],
   "source": [
    "# preprocess_job2 = Preprocess(x_t2, sen_len_job, w2v_path=\"word2vec_model/word2vec1.model\")\n",
    "# embedding2 = preprocess_job2.make_embedding(load=True)\n",
    "job2 = torch.LongTensor(x_t2)\n",
    "\n",
    "preprocess_job3 = Preprocess1(x_t3, sen_len_job, w2v_path=\"word2vec_model/word2vec_shared.model\")\n",
    "embedding3 = preprocess_job3.make_embedding(load=True)\n",
    "job3 = preprocess_job3.sentencelist_2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875/2263902633.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  self.embedding_matrix.append(self.embedding[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 137703\n"
     ]
    }
   ],
   "source": [
    "sen_len_user = 200\n",
    "preprocess_user1 = Preprocess(user_t1, sen_len_user, w2v_path=\"word2vec_model/word2vec_shared.model\")\n",
    "embedding4 = preprocess_user1.make_embedding(load=True)\n",
    "user1 = preprocess_user1.sentence_word2idx()\n",
    "\n",
    "# preprocess_user2 = Preprocess(user_t1, sen_len_user, w2v_path=\"word2vec_model/word2vec2.model\")\n",
    "# embedding5 = preprocess_user2.make_embedding(load=True)\n",
    "user2 = torch.LongTensor(user_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get embedding ...\n",
      "loading word2vec model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875/1316519862.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  self.embedding_matrix.append(self.embedding[word])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 137703\n"
     ]
    }
   ],
   "source": [
    "preprocess_user3 = Preprocess1(user_t1, sen_len_user, w2v_path=\"word2vec_model/word2vec_shared.model\")\n",
    "embedding6 = preprocess_user3.make_embedding(load=True)\n",
    "user3 = preprocess_user3.sentencelist_2idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t1 = [int (num) for num in y_t]\n",
    "y = torch.LongTensor(y_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_val_split(x1,x2,x3,x4,x5,x6,y, ratio_train, ratio_test, ratio_val):\n",
    "    x1_train, x1_middle,x2_train, x2_middle,x3_train, x3_middle,x4_train, x4_middle,x5_train, x5_middle,x6_train, x6_middle,y_train, y_middle = train_test_split(x1,x2,x3,x4,x5,x6,y, test_size=1-ratio_train, random_state=20)\n",
    "    ratio = ratio_val/(ratio_test + ratio_val)\n",
    "    x1_test, x1_validation,x2_test, x2_validation,x3_test, x3_validation,x4_test, x4_validation,x5_test, x5_validation,x6_test, x6_validation,y_test, y_validation = train_test_split(x1_middle,x2_middle,x3_middle,x4_middle,x5_middle,x6_middle,y_middle, test_size=ratio, random_state=20)\n",
    "    return x1_train, x1_test, x1_validation,x2_train, x2_test, x2_validation,x3_train, x3_test, x3_validation,x4_train, x4_test, x4_validation,x5_train, x5_test, x5_validation,x6_train, x6_test, x6_validation,y_train, y_test, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x1_train, x1_test, x1_validation,x2_train, x2_test, x2_validation,x3_train, x3_test, x3_validation,x4_train, x4_test, x4_validation,x5_train, x5_test, x5_validation,x6_train, x6_test, x6_validation,y_train, y_test, y_validation = train_test_val_split(job1,job2,job3,user1,user2,user3,y, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# dataset构建\n",
    "train_dataset = JobUserDataset(x1_train, x2_train, x3_train,x4_train,x5_train,x6_train,y_train)\n",
    "val_dataset = JobUserDataset(x1_test, x2_test, x3_test,x4_test,x5_test,x6_test,y_test)\n",
    "test_dataset = JobUserDataset(x1_validation, x2_validation, x3_validation,x4_validation,x5_validation,x6_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储dataset\n",
    "torch.save(train_dataset, \"baseline_dataset/train_1.dataset\")\n",
    "torch.save(val_dataset, \"baseline_dataset/val_1.dataset\")\n",
    "torch.save(test_dataset, \"baseline_dataset/test_1.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入dataset\n",
    "train_dataset = torch.load(\"baseline_dataset/train_1.dataset\")\n",
    "val_dataset = torch.load(\"baseline_dataset/val_1.dataset\")\n",
    "test_dataset = torch.load(\"baseline_dataset/test_1.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # 一次训练所选取的样本数\n",
    "# dataset导入\n",
    "train_loader = DataLoader(dataset= train_dataset, batch_size = batch_size, shuffle = False)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False)\n",
    "test_loader =DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class JRMPM(torch.nn.Module):\n",
    "    def __init__(self, word_embeddings):\n",
    "        super(JRMPM, self).__init__()\n",
    "\n",
    "        # profile: word embeddings for look_up\n",
    "        # embedding_matrix = [[0...0], [...], ...[]]\n",
    "        self.word_embeddings = torch.nn.Embedding.from_pretrained(word_embeddings, padding_idx=0)\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "\n",
    "        # BI-GRU: int(USER_EMBED_DIM/2) * 2 = USER_EMBED_DIM\n",
    "        self.expect_words_gru = torch.nn.GRU(input_size=WORD_EMBED_DIM, hidden_size=int(USER_EMBED_DIM/2),\n",
    "                                    num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.job_words_gru = torch.nn.GRU(input_size=WORD_EMBED_DIM, hidden_size=int(USER_EMBED_DIM/2),\n",
    "                                    num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # GRU: USER_EMBED_DIM\n",
    "        self.expect_sent_gru = torch.nn.GRU(input_size=USER_EMBED_DIM, hidden_size=USER_EMBED_DIM,\n",
    "                                    num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.job_sent_gru = torch.nn.GRU(input_size=USER_EMBED_DIM, hidden_size=USER_EMBED_DIM,\n",
    "                                    num_layers=1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        # memory profiling\n",
    "        self.expect_momery = torch.nn.Embedding(MAX_PROFILELEN, USER_EMBED_DIM)\n",
    "        self.expect_momery.weight.requires_grad = True\n",
    "        self.job_momery = torch.nn.Embedding(MAX_PROFILELEN, USER_EMBED_DIM)\n",
    "        self.job_momery.weight.requires_grad = True\n",
    "\n",
    "\n",
    "        # update pi: beta, gamma\n",
    "        self.expect_update_pi = torch.nn.Sequential(\n",
    "            torch.nn.Linear(MAX_PROFILELEN, MAX_PROFILELEN, bias=False),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Softmax(dim=-2)\n",
    "        )\n",
    "        self.job_update_pi = torch.nn.Sequential(\n",
    "            torch.nn.Linear( MAX_PROFILELEN,  MAX_PROFILELEN, bias=False),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Softmax(dim=-2)\n",
    "        )\n",
    "\n",
    "        # update g:\n",
    "        self.expect_g_update = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * USER_EMBED_DIM, 1, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.job_g_update = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * USER_EMBED_DIM, 1, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # read phi: alpha\n",
    "        self.expect_read_phi = torch.nn.Sequential(\n",
    "            torch.nn.Linear(MAX_PROFILELEN, MAX_PROFILELEN, bias=False),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Softmax(dim=-2)\n",
    "        )\n",
    "        self.job_read_phi = torch.nn.Sequential(\n",
    "            torch.nn.Linear(MAX_PROFILELEN, MAX_PROFILELEN, bias=False),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Softmax(dim=-2)\n",
    "        )\n",
    "        # read g:\n",
    "        self.expect_g_read = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * USER_EMBED_DIM, 1, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        self.job_g_read = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3 * USER_EMBED_DIM, 1, bias=False),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # match\n",
    "        self.MLP = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * MAX_PROFILELEN * USER_EMBED_DIM,  MAX_PROFILELEN * USER_EMBED_DIM),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(MAX_PROFILELEN * USER_EMBED_DIM, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # profiles: [batch_size, MAX_PROFILELEN, MAX_TERMLEN] = (40, 15, 50), word idx\n",
    "    def __words_BiGRU__(self, profiles, isexpect=True):\n",
    "        # word level:\n",
    "        shape = profiles.shape # [132, 20, 50]\n",
    "        profiles_ = profiles.contiguous().view([-1, shape[-1]])\n",
    "        # sort expects_sample_: large to small\n",
    "        # sorted [batch_size * MAX_PROFILELEN, MAX_TERMLEN](40 * 15, 50)\n",
    "        lens = (profiles_ > 0).sum(dim=-1)\n",
    "        lens_sort, ind_sort = lens.sort(dim=0, descending=True)\n",
    "        profiles_sort = profiles_[ind_sort]\n",
    "        # embeddings: [batch_size * MAX_PROFILELEN, MAX_TERMLEN, EMBED_DIM]\n",
    "        profile_embed = self.word_embeddings(profiles_sort).float()\n",
    "        profile_pack = pack_padded_sequence(profile_embed, lens_sort.cpu(), batch_first=True)\n",
    "        if isexpect:\n",
    "            _, sent_hidden = self.expect_words_gru(profile_pack)\n",
    "        else:\n",
    "            _, sent_hidden = self.job_words_gru(profile_pack)\n",
    "        # [2640, 2, 50]\n",
    "        sent_hidden = sent_hidden.permute(1, 0, 2).contiguous().view([-1, USER_EMBED_DIM])\n",
    "        sent_hidden = sent_hidden[ind_sort].view([shape[0], shape[1], -1])\n",
    "        # [132, 20, 100]\n",
    "        return sent_hidden\n",
    "\n",
    "    # sents: [batch_size, MAX_PROFILELEN, dim]\n",
    "    def __sents_GRU__(self, sent_hidden, isexpect=True):\n",
    "        if isexpect:\n",
    "            out, _ = self.expect_sent_gru(sent_hidden)\n",
    "        else:\n",
    "            out, _ = self.job_sent_gru(sent_hidden)\n",
    "        return out\n",
    "\n",
    "    def profile2sent(self, profiles, isexpect):\n",
    "        return self.__sents_GRU__(self.__words_BiGRU__(profiles, isexpect), isexpect)\n",
    "\n",
    "    # memory:  [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "    # a_sents: [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "    # b_sents: [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "    # col_mask: [batch]\n",
    "    def update(self, memory, a_sents, b_sents, col_mask, isexpect=True):\n",
    "        if isexpect:\n",
    "            # [batch, n, n*]\n",
    "            # print(torch.bmm(memory, a_sents.permute(0, 2, 1)).size())\n",
    "            beta = self.expect_update_pi(torch.bmm(memory, a_sents.permute(0, 2, 1)))\n",
    "            gamma = self.expect_update_pi(torch.bmm(memory, b_sents.permute(0, 2, 1)))\n",
    "        else:\n",
    "            beta = self.job_update_pi(torch.bmm(memory, a_sents.permute(0, 2, 1)))\n",
    "            gamma = self.job_update_pi(torch.bmm(memory, b_sents.permute(0, 2, 1)))\n",
    "\n",
    "        # [batch, n, n*] * [batch, n, dim] = [batch, n, dim]\n",
    "        i_update = torch.bmm(beta, a_sents) + torch.bmm(gamma, b_sents)\n",
    "        # [batch, n, dim]\n",
    "        if isexpect:\n",
    "            g_update = self.expect_g_update(torch.cat([memory, i_update, memory * i_update], dim=-1))\n",
    "        else:\n",
    "            g_update = self.job_g_update(torch.cat([memory, i_update, memory * i_update], dim=-1))\n",
    "        # m_{k+1}\n",
    "        # [batch, MAX_PROFILELEN, USER_EMBED_DIM]\n",
    "        memory_update = g_update * memory + (1-g_update) * memory\n",
    "        # mask\n",
    "        shape = memory_update.shape\n",
    "        memory_update_mask = (torch.unsqueeze(col_mask, 1) * memory_update.view([shape[0], -1])).view(shape)\n",
    "        memory_noupdate_mask = (torch.unsqueeze(1.-col_mask, 1) * memory.contiguous().view([shape[0], -1])).view(shape)\n",
    "        return memory_update_mask + memory_noupdate_mask\n",
    "\n",
    "    # memory: [batch, n, dim] [1, 20, 100]\n",
    "    # hidden_last: [batch, n, dim] [1, 20, 100]\n",
    "    # a_sents: [batch, n, dim] [1, 20, 100]\n",
    "    def read(self, memory, hidden_last, a_sents, isexpect=True):\n",
    "        # [batch, n, n*]\n",
    "        if isexpect:\n",
    "            alpha = self.expect_read_phi(torch.bmm(memory, (hidden_last * a_sents).permute(0, 2, 1)))\n",
    "        else:\n",
    "            alpha = self.job_read_phi(torch.bmm(memory, (hidden_last * a_sents).permute(0, 2, 1)))\n",
    "\n",
    "        # [batch, n, n*] * [batch, n, dim] = [batch, n, dim]\n",
    "        i_read = torch.bmm(alpha, memory)\n",
    "        # [batch, n, dim],\n",
    "        if isexpect:\n",
    "            g_read = self.expect_g_read(torch.cat([a_sents, i_read, a_sents * i_read], dim=-1))\n",
    "        else:\n",
    "            g_read = self.job_g_read(torch.cat([a_sents, i_read, a_sents * i_read], dim=-1))\n",
    "\n",
    "        # [batch, n, dim]\n",
    "        hidden = g_read * i_read + (1 - g_read) * hidden_last\n",
    "        return hidden\n",
    "\n",
    "    # a_profiles: [batch, sent, word] [1, 20, 50], tensor\n",
    "    # b_profiless: [batch, max_seq_len, sent, word] [1, 3, 20, 50], tensor\n",
    "    # b_seq_lens: [], list\n",
    "    def process_seq(self, a_profiles, b_seq_profiless, b_seq_lens, isexpect=True):\n",
    "        # [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "        batch_a_sents = self.profile2sent(a_profiles, isexpect)\n",
    "        # [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "        batch_memory = batch_hidden = self.profile2sent(a_profiles, not isexpect)\n",
    "        # print(max(b_seq_lens))\n",
    "        for i in range(10):\n",
    "            # [1,0,... ]\n",
    "            col_mask = torch.from_numpy((np.array(b_seq_lens.cpu())-i>0)+0.).float().to(device)\n",
    "            # col_mask = torch.Tensor((np.array(10)-i>0)+0.).float().to(device)\n",
    "            # [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "            batch_b_sents = self.profile2sent(b_seq_profiless[:, i, :, :], not isexpect)\n",
    "            # batch_memory:  [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "            # batch_a_sents: [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "            # batch_b_sents: [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "            # batch_memory:  [batch, MAX_PROFILELEN, USER_EMBED_DIM] [1, 20, 100]\n",
    "            batch_memory = self.update(batch_memory, batch_a_sents,batch_b_sents, col_mask, isexpect)\n",
    "            batch_hidden = self.read(batch_memory, batch_hidden, batch_a_sents, isexpect)\n",
    "\n",
    "        return batch_hidden\n",
    "\n",
    "    # [100, 20, 100] [100, 20, 100]\n",
    "    def predict(self, expect_hidden, job_hidden):\n",
    "        expect_hidden_ = expect_hidden.reshape([expect_hidden.shape[0], -1])\n",
    "        job_hidden_ = job_hidden.reshape([job_hidden.shape[0], -1])\n",
    "        return self.MLP(torch.cat([expect_hidden_, job_hidden_], -1))\n",
    "\n",
    "    def forward(self, job1, job2, job3, user1, user2, user3):\n",
    "        job_hidden = self.process_seq(job1.long().unsqueeze(1), job3.long().unsqueeze(2), job2.long())\n",
    "        user_hidden = self.process_seq(user1.long().unsqueeze(1), user3.long().unsqueeze(2), user2.long())\n",
    "        x = self.predict(job_hidden, user_hidden).squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def training(n_epoch, lr, train, valid, model, device, model_name, model_dir=\"./\"):\n",
    "    # summary model parameters\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(\"\\nstart training, total parameter:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    model.cuda()\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    t_batch = len(train)\n",
    "    v_batch = len(valid)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) #, weight_decay=1e-4\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch, eta_min=0, last_epoch=-1)\n",
    "    # total_loss, total_acc = 0, 0\n",
    "    best_acc, best_precision, best_recall, best_f1, best_auc = 0, 0, 0, 0, 0\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start_time = time.time()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        pred_label = []\n",
    "        y_label = []\n",
    "        # training\n",
    "        for i, (jobs1, jobs2, jobs3, users1, users2, users3,labels) in enumerate(valid):\n",
    "            # 放GPU上运行\n",
    "            jobs1 = jobs1.to(torch.float32)\n",
    "            jobs1 = jobs1.to(device)\n",
    "\n",
    "            jobs2 = jobs2.to(torch.float32)\n",
    "            jobs2 = jobs2.to(device)\n",
    "\n",
    "            jobs3 = jobs3.to(torch.float32)\n",
    "            jobs3 = jobs3.to(device)\n",
    "\n",
    "            users1 = users1.to(torch.float32)\n",
    "            users1 = users1.to(device)\n",
    "\n",
    "            users2 = users2.to(torch.float32)\n",
    "            users2 = users2.to(device)\n",
    "\n",
    "            users3 = users3.to(torch.float32)\n",
    "            users3 = users3.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # TODO 是否考虑模型用多个优化器？\n",
    "            optimizer.zero_grad() # 将所有模型参数的梯度置为0\n",
    "            # model.zero_grad() # 除所有可训练的torch.Tensor的梯度\n",
    "            outputs = model(jobs1, jobs2, jobs3, users1, users2, users3)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pred_label.extend([0 if i<0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "        train_losses = total_loss/t_batch\n",
    "        train_acc = accuracy_score(y_label, pred_label)\n",
    "        train_precision = precision_score(y_label, pred_label)\n",
    "        train_recall = recall_score(y_label, pred_label)\n",
    "        train_auc = roc_auc_score(y_label, pred_label)\n",
    "        train_f1 = f1_score(y_label, pred_label)\n",
    "        print('[ Epoch{}: {}/{}] '.format(epoch+1, i+1, t_batch))\n",
    "        print('\\nTrain | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(train_losses,train_acc,train_precision, train_recall,train_auc,train_f1, time.time()-start_time))\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # pred_score = []\n",
    "            pred_label = []\n",
    "            y_label = []\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (jobs1, jobs2, jobs3, users1, users2, users3,labels) in enumerate(valid):\n",
    "                # 放GPU上运行\n",
    "                jobs1 = jobs1.to(torch.float32)\n",
    "                jobs1 = jobs1.to(device)\n",
    "\n",
    "                jobs2 = jobs2.to(torch.float32)\n",
    "                jobs2 = jobs2.to(device)\n",
    "\n",
    "                jobs3 = jobs3.to(torch.float32)\n",
    "                jobs3 = jobs3.to(device)\n",
    "\n",
    "                users1 = users1.to(torch.float32)\n",
    "                users1 = users1.to(device)\n",
    "\n",
    "                users2 = users2.to(torch.float32)\n",
    "                users2 = users2.to(device)\n",
    "\n",
    "                users3 = users3.to(torch.float32)\n",
    "                users3 = users3.to(device)\n",
    "\n",
    "                labels = labels.to(torch.float32)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(jobs1, jobs2, jobs3, users1, users2, users3)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                '''\n",
    "                存一下预测score\n",
    "                '''\n",
    "                # pred_score.extend([j for j in list(outputs.cpu().detach().numpy())])\n",
    "                pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "                y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "\n",
    "            val_losses = total_loss/v_batch\n",
    "            val_acc = accuracy_score(y_label, pred_label)\n",
    "            val_precision = precision_score(y_label, pred_label)\n",
    "            val_recall = recall_score(y_label, pred_label)\n",
    "            val_auc = roc_auc_score(y_label, pred_label)\n",
    "            val_f1 = f1_score(y_label, pred_label)\n",
    "            print('\\nVal | Loss:{:.5f} ACC:{:.5f} Precision:{:.5f} Recall:{:.5f} AUC:{:.5f} F1:{:.5f} Time:{:.6f}'.format(val_losses,val_acc,val_precision, val_recall,val_auc,val_f1, time.time()-start_time))\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_precision = val_precision\n",
    "                best_recall = val_recall\n",
    "                best_f1 = val_f1\n",
    "                best_auc = val_auc\n",
    "                torch.save(model, \"{}/{}.model\".format(model_dir, model_name))\n",
    "                print('save model with acc: {:.3f}, recall: {:.3f}, auc: {:.3f}'.format(best_acc,best_recall,best_auc))\n",
    "        print('------------------------------------------------------')\n",
    "        # lr_scheduler.step()\n",
    "        # 将model的模式设为train，这样optimizer就可以更新model的參數（因為刚刚转为eval模式）\n",
    "        model.train()\n",
    "    return best_acc, best_precision, best_recall, best_f1, best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "WORD_EMBED_DIM = 200\n",
    "USER_EMBED_DIM = 200\n",
    "\n",
    "MAX_PROFILELEN = 1\n",
    "MAX_TERMLEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fix_embedding = False\n",
    "# input_dim = train_dataset[0][1].shape[0]\n",
    "model = JRMPM(embedding1)\n",
    "epoch = 20\n",
    "lr = 0.001\n",
    "model_dir = './'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'JRMPM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "start training, total parameter:28468605, trainable:928005\n",
      "\n",
      "[ Epoch1: 864/2592] \n",
      "\n",
      "Train | Loss:0.23125 ACC:0.49949 Precision:0.50094 Recall:0.59687 AUC:0.49918 F1:0.54471 Time:278.990605\n",
      "\n",
      "Val | Loss:0.69313 ACC:0.50130 Precision:0.50147 Recall:0.99560 AUC:0.49969 F1:0.66699 Time:357.354973\n",
      "save model with acc: 0.501, recall: 0.996, auc: 0.500\n",
      "------------------------------------------------------\n",
      "[ Epoch2: 864/2592] \n",
      "\n",
      "Train | Loss:0.23099 ACC:0.50958 Precision:0.50930 Recall:0.61180 AUC:0.50925 F1:0.55586 Time:271.856415\n",
      "\n",
      "Val | Loss:0.69305 ACC:0.50539 Precision:0.50360 Recall:0.97866 AUC:0.50384 F1:0.66500 Time:350.289876\n",
      "save model with acc: 0.505, recall: 0.979, auc: 0.504\n",
      "------------------------------------------------------\n",
      "[ Epoch3: 864/2592] \n",
      "\n",
      "Train | Loss:0.23051 ACC:0.51450 Precision:0.51317 Recall:0.62665 AUC:0.51414 F1:0.56426 Time:273.124255\n",
      "\n",
      "Val | Loss:0.69289 ACC:0.51837 Precision:0.51389 Recall:0.73740 AUC:0.51766 F1:0.60569 Time:351.709325\n",
      "save model with acc: 0.518, recall: 0.737, auc: 0.518\n",
      "------------------------------------------------------\n",
      "[ Epoch4: 864/2592] \n",
      "\n",
      "Train | Loss:0.22871 ACC:0.52734 Precision:0.52705 Recall:0.56269 AUC:0.52723 F1:0.54429 Time:276.424760\n",
      "\n",
      "Val | Loss:0.69199 ACC:0.52861 Precision:0.52664 Recall:0.59586 AUC:0.52839 F1:0.55912 Time:354.759636\n",
      "save model with acc: 0.529, recall: 0.596, auc: 0.528\n",
      "------------------------------------------------------\n",
      "[ Epoch5: 864/2592] \n",
      "\n",
      "Train | Loss:0.22658 ACC:0.54344 Precision:0.54385 Recall:0.55707 AUC:0.54339 F1:0.55038 Time:274.478401\n",
      "\n",
      "Val | Loss:0.69177 ACC:0.52514 Precision:0.52417 Recall:0.57856 AUC:0.52496 F1:0.55002 Time:352.892617\n",
      "------------------------------------------------------\n",
      "[ Epoch6: 864/2592] \n",
      "\n",
      "Train | Loss:0.22373 ACC:0.56029 Precision:0.55654 Recall:0.60754 AUC:0.56014 F1:0.58092 Time:264.403985\n",
      "\n",
      "Val | Loss:0.69101 ACC:0.52984 Precision:0.53211 Recall:0.51972 AUC:0.52987 F1:0.52584 Time:342.215645\n",
      "save model with acc: 0.530, recall: 0.520, auc: 0.530\n",
      "------------------------------------------------------\n",
      "[ Epoch7: 864/2592] \n",
      "\n",
      "Train | Loss:0.22246 ACC:0.57100 Precision:0.57084 Recall:0.58332 AUC:0.57096 F1:0.57701 Time:265.114328\n",
      "\n",
      "Val | Loss:0.68859 ACC:0.54011 Precision:0.55357 Recall:0.42988 AUC:0.54047 F1:0.48395 Time:342.802941\n",
      "save model with acc: 0.540, recall: 0.430, auc: 0.540\n",
      "------------------------------------------------------\n",
      "[ Epoch8: 864/2592] \n",
      "\n",
      "Train | Loss:0.22170 ACC:0.57386 Precision:0.57620 Recall:0.56897 AUC:0.57387 F1:0.57256 Time:274.516382\n",
      "\n",
      "Val | Loss:0.68741 ACC:0.53964 Precision:0.56516 Recall:0.35677 AUC:0.54024 F1:0.43741 Time:355.466385\n",
      "------------------------------------------------------\n",
      "[ Epoch9: 864/2592] \n",
      "\n",
      "Train | Loss:0.22167 ACC:0.57277 Precision:0.57798 Recall:0.54964 AUC:0.57285 F1:0.56346 Time:273.793249\n",
      "\n",
      "Val | Loss:0.68618 ACC:0.54510 Precision:0.56642 Recall:0.39722 AUC:0.54559 F1:0.46696 Time:352.658110\n",
      "save model with acc: 0.545, recall: 0.397, auc: 0.546\n",
      "------------------------------------------------------\n",
      "[ Epoch10: 864/2592] \n",
      "\n",
      "Train | Loss:0.21636 ACC:0.59346 Precision:0.59523 Recall:0.59240 AUC:0.59346 F1:0.59381 Time:267.854967\n",
      "\n",
      "Val | Loss:0.68482 ACC:0.55031 Precision:0.57310 Recall:0.40587 AUC:0.55078 F1:0.47520 Time:346.233509\n",
      "save model with acc: 0.550, recall: 0.406, auc: 0.551\n",
      "------------------------------------------------------\n",
      "[ Epoch11: 864/2592] \n",
      "\n",
      "Train | Loss:0.21025 ACC:0.61654 Precision:0.61848 Recall:0.61482 AUC:0.61654 F1:0.61665 Time:269.149104\n",
      "\n",
      "Val | Loss:0.68533 ACC:0.55349 Precision:0.57629 Recall:0.41503 AUC:0.55395 F1:0.48254 Time:348.435447\n",
      "save model with acc: 0.553, recall: 0.415, auc: 0.554\n",
      "------------------------------------------------------\n",
      "[ Epoch12: 864/2592] \n",
      "\n",
      "Train | Loss:0.20565 ACC:0.63220 Precision:0.63398 Recall:0.63119 AUC:0.63220 F1:0.63258 Time:269.048678\n",
      "\n",
      "Val | Loss:0.68591 ACC:0.56047 Precision:0.58171 Recall:0.44070 AUC:0.56087 F1:0.50148 Time:348.632690\n",
      "save model with acc: 0.560, recall: 0.441, auc: 0.561\n",
      "------------------------------------------------------\n",
      "[ Epoch13: 864/2592] \n",
      "\n",
      "Train | Loss:0.19947 ACC:0.65571 Precision:0.65454 Recall:0.66422 AUC:0.65568 F1:0.65934 Time:269.158037\n",
      "\n",
      "Val | Loss:0.69081 ACC:0.56055 Precision:0.58524 Recall:0.42548 AUC:0.56099 F1:0.49274 Time:348.307066\n",
      "save model with acc: 0.561, recall: 0.425, auc: 0.561\n",
      "------------------------------------------------------\n",
      "[ Epoch14: 864/2592] \n",
      "\n",
      "Train | Loss:0.19559 ACC:0.66562 Precision:0.66812 Recall:0.66248 AUC:0.66563 F1:0.66529 Time:270.516391\n",
      "\n",
      "Val | Loss:0.69510 ACC:0.56702 Precision:0.58849 Recall:0.45504 AUC:0.56739 F1:0.51324 Time:351.092401\n",
      "save model with acc: 0.567, recall: 0.455, auc: 0.567\n",
      "------------------------------------------------------\n",
      "[ Epoch15: 864/2592] \n",
      "\n",
      "Train | Loss:0.19262 ACC:0.67603 Precision:0.67810 Recall:0.67424 AUC:0.67604 F1:0.67616 Time:276.363587\n",
      "\n",
      "Val | Loss:0.71690 ACC:0.55834 Precision:0.59990 Recall:0.35893 AUC:0.55899 F1:0.44914 Time:354.703279\n",
      "------------------------------------------------------\n",
      "[ Epoch16: 864/2592] \n",
      "\n",
      "Train | Loss:0.18580 ACC:0.69578 Precision:0.69532 Recall:0.70048 AUC:0.69577 F1:0.69789 Time:260.080500\n",
      "\n",
      "Val | Loss:0.73603 ACC:0.56189 Precision:0.60011 Recall:0.37948 AUC:0.56248 F1:0.46495 Time:338.158454\n",
      "------------------------------------------------------\n",
      "[ Epoch17: 864/2592] \n",
      "\n",
      "Train | Loss:0.17913 ACC:0.71448 Precision:0.71286 Recall:0.72139 AUC:0.71446 F1:0.71710 Time:272.121599\n",
      "\n",
      "Val | Loss:0.75609 ACC:0.56651 Precision:0.60204 Recall:0.40075 AUC:0.56706 F1:0.48119 Time:350.201483\n",
      "------------------------------------------------------\n",
      "[ Epoch18: 864/2592] \n",
      "\n",
      "Train | Loss:0.17444 ACC:0.72765 Precision:0.72580 Recall:0.73459 AUC:0.72762 F1:0.73017 Time:254.855622\n",
      "\n",
      "Val | Loss:0.75095 ACC:0.56688 Precision:0.59268 Recall:0.43666 AUC:0.56730 F1:0.50284 Time:333.550407\n",
      "------------------------------------------------------\n",
      "[ Epoch19: 864/2592] \n",
      "\n",
      "Train | Loss:0.16955 ACC:0.73492 Precision:0.73344 Recall:0.74079 AUC:0.73490 F1:0.73710 Time:259.414618\n",
      "\n",
      "Val | Loss:0.74565 ACC:0.57885 Precision:0.60336 Recall:0.46824 AUC:0.57921 F1:0.52728 Time:339.129278\n",
      "save model with acc: 0.579, recall: 0.468, auc: 0.579\n",
      "------------------------------------------------------\n",
      "[ Epoch20: 864/2592] \n",
      "\n",
      "Train | Loss:0.16279 ACC:0.75094 Precision:0.74923 Recall:0.75680 AUC:0.75092 F1:0.75300 Time:270.685219\n",
      "\n",
      "Val | Loss:0.75807 ACC:0.57838 Precision:0.60515 Recall:0.45894 AUC:0.57877 F1:0.52200 Time:348.376447\n",
      "------------------------------------------------------\n",
      "best_acc 0.5788483796296297\n",
      "best_precision 0.6033633745238316\n",
      "best_recall 0.6033633745238316\n",
      "best_f1 0.5272815849301722\n",
      "best_auc 0.5792096136664439\n"
     ]
    }
   ],
   "source": [
    "best_acc, best_precision, best_recall, best_f1, best_auc = training(epoch, lr, train_loader, val_loader, model, device, model_name, model_dir)\n",
    "\n",
    "# 输出结果（验证集）\n",
    "print('best_acc',best_acc)\n",
    "print('best_precision',best_precision)\n",
    "print('best_recall',best_precision)\n",
    "print('best_f1',best_f1)\n",
    "print('best_auc',best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, test_loader):\n",
    "    pred_label = []\n",
    "    y_label = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (jobs1, jobs2, jobs3, users1, users2, users3,labels) in enumerate(test_loader):\n",
    "            # 放GPU上运行\n",
    "            jobs1 = jobs1.to(torch.float32)\n",
    "            jobs1 = jobs1.to(device)\n",
    "\n",
    "            jobs2 = jobs2.to(torch.float32)\n",
    "            jobs2 = jobs2.to(device)\n",
    "\n",
    "            jobs3 = jobs3.to(torch.float32)\n",
    "            jobs3 = jobs3.to(device)\n",
    "\n",
    "            users1 = users1.to(torch.float32)\n",
    "            users1 = users1.to(device)\n",
    "\n",
    "            users2 = users2.to(torch.float32)\n",
    "            users2 = users2.to(device)\n",
    "\n",
    "            users3 = users3.to(torch.float32)\n",
    "            users3 = users3.to(device)\n",
    "\n",
    "            labels = labels.to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(jobs1, jobs2, jobs3, users1, users2, users3)\n",
    "\n",
    "            pred_label.extend([0 if i < 0.5 else 1 for i in list(outputs.cpu().detach().numpy())])\n",
    "            y_label.extend(list(labels.cpu().detach().numpy()))\n",
    "\n",
    "        test_acc = accuracy_score(y_label, pred_label)\n",
    "        test_precision = precision_score(y_label, pred_label)\n",
    "        test_recall = recall_score(y_label, pred_label)\n",
    "        test_auc = roc_auc_score(y_label, pred_label)\n",
    "        test_f1 = f1_score(y_label, pred_label)\n",
    "    return test_acc, test_auc, test_precision, test_recall, test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc 0.5002170138888888\n",
      "test_precision 0.4953673373888629\n",
      "test_recall 0.4953673373888629\n",
      "test_f1 0.4337813473201115\n",
      "test_auc 0.4993546274484371\n"
     ]
    }
   ],
   "source": [
    "# 输出结果(测试集)\n",
    "test_acc, test_auc, test_precision, test_recall, test_f1 = testing(\n",
    "    torch.load('JRMPM.model'), test_loader)\n",
    "print('test_acc', test_acc)\n",
    "print('test_precision', test_precision)\n",
    "print('test_recall', test_precision)\n",
    "print('test_f1', test_f1)\n",
    "print('test_auc', test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
